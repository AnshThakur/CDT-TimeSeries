{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sw9jhQgQxIU6"
   },
   "source": [
    "## State Space Models \n",
    "# Major Takeaways:\n",
    "  - This notebook demonstrates a simple State Space Model (SSM) for time series modeling.\n",
    "Unlike RNNs or LSTMs, the SSM computes its output using parallel 1D convolutions instead of sequential unrolling through time, showcasing improved computational efficiency.\n",
    "  - The implemented SSM is a minimal linear variant with a diagonal state matrix, so it serves as a baseline model. We expect it to have lower modeling capacity compared to nonlinear RNNs or LSTMs.\n",
    "  - We will train both SSM and RNN models on the same dataset and analyze the differences in their performance, learning dynamics, and computational efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1188,
     "status": "ok",
     "timestamp": 1636212379714,
     "user": {
      "displayName": "Anshul Thakur",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11719449395424920327"
     },
     "user_tz": 0
    },
    "id": "8Ust8VKMxIU7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from utils import get_validation_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSMLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Diagonal State Space Model (SSM) layer.\n",
    "\n",
    "    Params:\n",
    "      - n: hidden/state dimension (size of diagonal A)\n",
    "      - d_in: input dimension (per timestep)\n",
    "      - d_out: output dimension (per timestep)\n",
    "      - max_len: maximum sequence length supported (used for kernel computation)\n",
    "\n",
    "    Input to forward: x with shape (batch, d_in, T)\n",
    "    Output: y with shape (batch, d_out, T)\n",
    "    \"\"\"\n",
    "    def __init__(self, n, d_in, d_out, max_len=512):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # unconstrained param -> squashed to (-1,1) for stability\n",
    "        self.logit_a = nn.Parameter(-3.0 * torch.ones(n))\n",
    "        self.B = nn.Parameter(torch.randn(n, d_in) * 0.1)     # (n, d_in)\n",
    "        self.C = nn.Parameter(torch.randn(d_out, n) * 0.1)    # (d_out, n)\n",
    "        self.D = nn.Parameter(torch.zeros(d_out, d_in))      # (d_out, d_in)\n",
    "\n",
    "    def _get_a(self):\n",
    "        # keep eigenvalues in (-1,1)\n",
    "        # optionally multiply by 0.999 to avoid exact Â±1\n",
    "        return 0.999 * torch.tanh(self.logit_a)\n",
    "\n",
    "    def compute_kernel(self, T):\n",
    "        \"\"\"Return K with shape (d_out, d_in, T).\"\"\"\n",
    "        if T > self.max_len:\n",
    "            self.max_len = T\n",
    "\n",
    "        a = self._get_a()                           # (n,)\n",
    "        ks = torch.arange(T, dtype=a.dtype, device=a.device)   # (T,)\n",
    "        powers = a.unsqueeze(0).pow(ks.unsqueeze(1))           # (T, n)\n",
    "        AB = powers.unsqueeze(2) * self.B.unsqueeze(0)         # (T, n, d_in)\n",
    "        K_t = torch.einsum('on,tni->toi', self.C, AB)          # (T, d_out, d_in)\n",
    "        K_t[0] = K_t[0] + self.D                               # K[0] += D\n",
    "        K = K_t.permute(1, 2, 0).contiguous()                  # (d_out, d_in, T)\n",
    "        return K\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, d_in, T)\n",
    "        returns y: (batch, d_out, T)\n",
    "        \"\"\"\n",
    "        T = x.shape[-1]\n",
    "        K = self.compute_kernel(T)                    # (d_out, d_in, T)\n",
    "        K_rev = torch.flip(K, dims=[2])               # conv1d performs cross-correlation -> reverse kernel\n",
    "        pad = K_rev.shape[2] - 1\n",
    "        # Pad left with zeros to make conv causal and preserve length\n",
    "        x_padded = F.pad(x, (pad, 0))                 # pad=(left, right) for last dim\n",
    "        y = F.conv1d(x_padded, K_rev)                 # (batch, d_out, T)\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1636212379715,
     "user": {
      "displayName": "Anshul Thakur",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11719449395424920327"
     },
     "user_tz": 0
    },
    "id": "f39vXbYTxIU9"
   },
   "outputs": [],
   "source": [
    "class SSMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop-in replacement for your RNNClassifier:\n",
    "      - constructor signature: (input_dim, hidden_dim, device)\n",
    "      - forward(x) expects (batch, time_steps, features)\n",
    "      - returns sigmoid output of shape (batch, 1)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, device):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # note SSMLayer signature (n, d_in, d_out)\n",
    "        # we want per-timestep 'hidden' vectors of dim hidden_dim, so set d_out = hidden_dim\n",
    "        self.ssm = SSMLayer(hidden_dim, input_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, time_steps, features)\n",
    "        returns: (batch, 1) with sigmoid activation\n",
    "        \"\"\"\n",
    "        # Move to device if needed (optional if caller already does)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # Permute to conv format: (batch, features, time)\n",
    "        x_perm = x.permute(0, 2, 1).contiguous()   # (B, d_in, T)\n",
    "\n",
    "        # SSM forward -> outputs per timestep hidden vectors (B, hidden_dim, T)\n",
    "        y_seq = self.ssm(x_perm)                   # (B, hidden_dim, T)\n",
    "\n",
    "        # Choose how to map sequence -> single vector for classification:\n",
    "        # To mimic the RNNClassifier (which uses the last hidden state), pick last timestep:\n",
    "        hidden_last = y_seq[:, :, -1]              # (B, hidden_dim)\n",
    "\n",
    "        # Optionally you could do mean pooling: hidden_last = y_seq.mean(dim=2)\n",
    "\n",
    "        out = self.fc(hidden_last)                 # (B, 1)\n",
    "        return self.activation(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For comparison\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim,device):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn = nn.RNNCell(input_dim, hidden_dim)  # RNN Cell\n",
    "        self.fc = nn.Linear(hidden_dim, 1)            # fully connected layer: maps last hidden vector to model prediction\n",
    "        self.activation = nn.Sigmoid()                # coz binary classification\n",
    "        self.device=device\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        hidden = self.init_hidden(x)\n",
    "        \n",
    "        ############################# \n",
    "        \n",
    "        # Write you code here.\n",
    "        # Return expects variable out. Its the hidden vector obtained after last time-step.\n",
    "        \n",
    "        time_steps=x.shape[1]                 # shape of x is (batches,time_Steps,features)\n",
    "        \n",
    "        for i in range(0,time_steps):\n",
    "            inputs=x[:,i]                     # (batch,features) shape\n",
    "            hidden = self.rnn(inputs,hidden)\n",
    "            \n",
    "        out = self.fc(hidden)                 # take the hidden vector corresponding to last time step\n",
    "        ###########################\n",
    "        \n",
    "        return self.activation(out)\n",
    "    \n",
    "    def init_hidden(self, x):\n",
    "        h0 = torch.zeros(x.size(0), self.hidden_dim)\n",
    "        return h0.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1636212379717,
     "user": {
      "displayName": "Anshul Thakur",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11719449395424920327"
     },
     "user_tz": 0
    },
    "id": "Pdk6nqo8xIU-"
   },
   "outputs": [],
   "source": [
    "# Function to compute validation score: Used in trainer()\n",
    "\n",
    "def get_validation_score(model,Val_T,Val_L):\n",
    "    model.eval()\n",
    "    tensor_x = torch.Tensor(Val_T).to(model.device)\n",
    "    preds=model(tensor_x)[:,0]\n",
    "    LOSS=nn.BCELoss().to(device)\n",
    "    val_loss=LOSS(preds,torch.Tensor(Val_L).type(torch.FloatTensor).to(model.device))\n",
    "    return roc_auc_score(Val_L, preds.cpu().detach().numpy()), val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1636212379718,
     "user": {
      "displayName": "Anshul Thakur",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11719449395424920327"
     },
     "user_tz": 0
    },
    "id": "qjKFBLTyxIU_"
   },
   "outputs": [],
   "source": [
    "def trainer(model,training_set,validation_set,device,lr,stored_name,epochs=10):\n",
    "    \n",
    "    # Recieves data and labels \n",
    "    T,L=training_set \n",
    "    Val_T,Val_L=validation_set\n",
    "    \n",
    "    # intialise optimiser and criterion\n",
    "    \n",
    "    optimizer_model = torch.optim.SGD(model.parameters(),lr,momentum=0.9, nesterov=True)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "\n",
    "    # \n",
    "    best=0\n",
    "    LOSS=[]\n",
    "    VAL_LOSS=[]\n",
    "    \n",
    "    # training begins\n",
    "    \n",
    "    for epoch in range(0,epochs):\n",
    "        Loss=0\n",
    "        model.train()\n",
    "        for k in range(0,len(T)):\n",
    "            \n",
    "            inputs=T[k]\n",
    "            labels=L[k]\n",
    "            \n",
    "            inputs=torch.Tensor(inputs).to(device)\n",
    "            labels=torch.Tensor(labels).type(torch.FloatTensor).to(device)\n",
    "            \n",
    "            pred=model(inputs)\n",
    "            \n",
    "            loss=criterion(pred[:,0],labels)\n",
    "            optimizer_model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_model.step()\n",
    "            Loss=Loss+loss\n",
    "           \n",
    "        Val_ROC,val_loss=get_validation_score(model,Val_T,Val_L)\n",
    "        VAL_LOSS.append(val_loss.detach().cpu().numpy())\n",
    "        LOSS.append((Loss/len(T)).detach().cpu().numpy())\n",
    "        \n",
    "        print(' Epoch: {:.1f} Training Loss {:5f} Validation Loss {:.4f} Validation AUC {:.5f}'.format(epoch,LOSS[-1],VAL_LOSS[-1],Val_ROC))\n",
    "        \n",
    "        # If current validation score is greater than best, store the model\n",
    "        \n",
    "        if best<Val_ROC:\n",
    "           torch.save(model, './'+stored_name) \n",
    "\n",
    "    return torch.load('./'+stored_name).to(device),LOSS,VAL_LOSS     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 845,
     "status": "ok",
     "timestamp": 1636212380550,
     "user": {
      "displayName": "Anshul Thakur",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11719449395424920327"
     },
     "user_tz": 0
    },
    "id": "8itcyfT3xIVA",
    "outputId": "a8415f54-8b80-4f65-c715-a0d00320c399"
   },
   "outputs": [],
   "source": [
    "# Get training and validation data\n",
    "\n",
    "from get_data import get_training_data,get_validation_data\n",
    "\n",
    "T,L=get_training_data(batch_size=32)  # returns lists of training data and label batches\n",
    "Val_T,Val_L=get_validation_data()     # numpy arrays of validation data and labels\n",
    "\n",
    "print(T[0].shape)                     # (batch_size,time_steps,n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1636212380551,
     "user": {
      "displayName": "Anshul Thakur",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11719449395424920327"
     },
     "user_tz": 0
    },
    "id": "y43nY3esxIVC"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1636212380552,
     "user": {
      "displayName": "Anshul Thakur",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11719449395424920327"
     },
     "user_tz": 0
    },
    "id": "wrOne0ySxIVC"
   },
   "outputs": [],
   "source": [
    "n_features=T[0].shape[2]     # 76 dimensional vector at each time step\n",
    "recurrent_units=128          # number of hidden units in  a RNN/LSTM\n",
    "lr=0.001                     # learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1636212380553,
     "user": {
      "displayName": "Anshul Thakur",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11719449395424920327"
     },
     "user_tz": 0
    },
    "id": "NnxeESThxIVD",
    "outputId": "69d9927a-96aa-49ef-e904-e980717c0396"
   },
   "outputs": [],
   "source": [
    "# Create LSTMClassifier model object\n",
    "\n",
    "model=SSMClassifier(n_features,recurrent_units,device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82127,
     "status": "ok",
     "timestamp": 1636212462671,
     "user": {
      "displayName": "Anshul Thakur",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11719449395424920327"
     },
     "user_tz": 0
    },
    "id": "dGsbdDO9xIVD",
    "outputId": "9910184d-d6d3-4703-bda4-dfc8e44f0eda"
   },
   "outputs": [],
   "source": [
    "# Train SSM\n",
    "\n",
    "model=model.to(device)\n",
    "model, training_loss, validation_loss=trainer(model,(T,L),(Val_T,Val_L),device,lr,stored_name='ssm',epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1636212463051,
     "user": {
      "displayName": "Anshul Thakur",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11719449395424920327"
     },
     "user_tz": 0
    },
    "id": "dNz56XhfxIVE",
    "outputId": "96fea02f-60c8-47db-afe0-0571394183d8"
   },
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "lw = 2\n",
    "\n",
    "plt.tight_layout()\n",
    "ax.plot(np.linspace(1, len(training_loss), num=len(training_loss)),training_loss, color='rebeccapurple',\n",
    "         lw=2, linestyle='-', label='Training Loss')\n",
    "\n",
    "ax.plot(np.linspace(1, len(training_loss), num=len(training_loss)),validation_loss, color='r',\n",
    "         lw=2, linestyle='-', label='Validation_loss')\n",
    "\n",
    "ax.set_xlabel('# Epochs',fontsize=14)\n",
    "ax.set_ylabel('# Loss',fontsize=14)\n",
    "ax.legend(loc=\"best\",fontsize=12)\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=13)\n",
    "ax.tick_params(axis='y', labelsize=13)\n",
    "\n",
    "\n",
    "plt.grid(color='gray', linestyle='-', linewidth=0.1)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1636212463052,
     "user": {
      "displayName": "Anshul Thakur",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11719449395424920327"
     },
     "user_tz": 0
    },
    "id": "hDbJz7X5xIVE",
    "outputId": "f3f93b10-2890-4cb6-87f6-6444f86b35b4"
   },
   "outputs": [],
   "source": [
    "# Create RNNClassifier model object\n",
    "\n",
    "model=RNNClassifier(n_features,recurrent_units,device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25245,
     "status": "ok",
     "timestamp": 1636212488277,
     "user": {
      "displayName": "Anshul Thakur",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11719449395424920327"
     },
     "user_tz": 0
    },
    "id": "rfH4BX_TxIVE",
    "outputId": "61f99dbe-2714-4dee-f0fa-f13185452959"
   },
   "outputs": [],
   "source": [
    "# Train RNNClassifier\n",
    "\n",
    "model=model.to(device)\n",
    "model, training_loss, validation_loss=trainer(model,(T,L),(Val_T,Val_L),device,lr,stored_name='rnn',epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1636212488279,
     "user": {
      "displayName": "Anshul Thakur",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11719449395424920327"
     },
     "user_tz": 0
    },
    "id": "Mu9R8dUAxIVF",
    "outputId": "88a70b17-dff8-4eed-abd5-eaf2b9503f95"
   },
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "lw = 2\n",
    "\n",
    "plt.tight_layout()\n",
    "ax.plot(np.linspace(1, len(training_loss), num=len(training_loss)),training_loss, color='rebeccapurple',\n",
    "         lw=2, linestyle='-', label='Training Loss')\n",
    "\n",
    "ax.plot(np.linspace(1, len(training_loss), num=len(training_loss)),validation_loss, color='r',\n",
    "         lw=2, linestyle='-', label='Validation_loss')\n",
    "\n",
    "ax.set_xlabel('# Epochs',fontsize=14)\n",
    "ax.set_ylabel('# Loss',fontsize=14)\n",
    "ax.legend(loc=\"best\",fontsize=12)\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=13)\n",
    "ax.tick_params(axis='y', labelsize=13)\n",
    "\n",
    "\n",
    "plt.grid(color='gray', linestyle='-', linewidth=0.1)\n",
    "\n",
    "\n",
    "#plt.savefig('./loss_curves.pdf',dpi=100,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Problem 1  with solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c93591c313a95c8a89c667763bfd86bbb342a6e2dd3e91f10558630342cde4f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
