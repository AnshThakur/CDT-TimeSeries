{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Federated Learning?\n",
    "\n",
    "Federated Learning (FL) is a **decentralized machine learning approach** where multiple clients (e.g., hospitals, mobile devices, or institutions) collaboratively train a shared global model **without sharing their raw data**.\n",
    "\n",
    "Instead of pooling all data in a central server, each client:\n",
    "1. **Trains locally** on its own private dataset.\n",
    "2. **Sends model updates (parameters or gradients)** ‚Äî not the data ‚Äî to a central server.\n",
    "3. The **server aggregates** these updates to improve the **global model**.\n",
    "4. This process repeats over several **communication rounds** until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Why Federated Learning?\n",
    "\n",
    "Traditional centralized ML requires moving all data to one place ‚Äî often impractical or illegal due to **privacy**, **security**, or **data ownership** concerns.\n",
    "\n",
    "Federated Learning enables:\n",
    "- ‚úÖ **Data privacy** ‚Äî raw data never leaves the client.\n",
    "- ‚úÖ **Scalability** ‚Äî computation distributed across clients.\n",
    "- ‚úÖ **Collaboration** ‚Äî multiple institutions can contribute without sharing sensitive data.\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Federated Learning Workflow\n",
    "\n",
    "**1Ô∏è‚É£ Initialization (Server)**\n",
    "- The server defines a **global model** (e.g., CNN, LSTM).\n",
    "- Sends the model weights to all clients.\n",
    "\n",
    "**2Ô∏è‚É£ Local Training (Clients)**\n",
    "- Each client trains the model on its **local dataset** for a few epochs.\n",
    "- The resulting **updated model weights** are sent back to the server.\n",
    "\n",
    "**3Ô∏è‚É£ Aggregation (Server)**\n",
    "- The server combines all local updates (e.g., via **Federated Averaging ‚Äî FedAvg**).\n",
    "- Produces a new **global model** that captures knowledge from all clients.\n",
    "\n",
    "**4Ô∏è‚É£ Iteration**\n",
    "- The process repeats for multiple **federated rounds** until the global model converges.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Federated Averaging (FedAvg)\n",
    "\n",
    "FedAvg is the **core aggregation algorithm** in Federated Learning *(McMahan et al., 2017)*.  \n",
    "It computes the **weighted average of all client model parameters**:\n",
    "\n",
    "$$\n",
    "w_{t+1} = \\sum_{k=1}^{K} \\frac{n_k}{N} \\, w_{t+1}^{(k)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $w_{t+1}^{(k)}$: Model parameters from client *k*  \n",
    "- $n_k$: Number of training samples on client *k*  \n",
    "- $N = \\sum_k n_k$: Total number of samples across all clients  \n",
    "- $w_{t+1}$: Updated global model parameters  \n",
    "\n",
    "Clients with **more data** influence the global model **more**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è IID vs Non-IID Data\n",
    "\n",
    "In FL, client data distributions are often **non-IID** (not identically distributed):\n",
    "\n",
    "| Scenario | Description | Example |\n",
    "|-----------|--------------|----------|\n",
    "| **IID** | Clients have data drawn from similar distributions | Each hospital has balanced patient demographics |\n",
    "| **Non-IID** | Clients have biased or skewed data distributions | One hospital has mostly older patients, another mostly younger |\n",
    "\n",
    "Non-IID settings make FL more challenging since local models may diverge significantly ‚Äî proper aggregation and hyperparameter tuning are key.\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Evaluating FL Models\n",
    "\n",
    "After each round:\n",
    "- Each client‚Äôs **local model** can be evaluated on its **own validation/test set**.\n",
    "- The **global model** can also be evaluated on each client‚Äôs data to assess overall generalization.\n",
    "\n",
    "Common metrics:\n",
    "- **AUC (Area Under ROC Curve)** ‚Äî measures discrimination ability.\n",
    "- **APR (Average Precision Recall)** ‚Äî evaluates precision-recall trade-off.\n",
    "- **Loss (e.g., BCE, MSE)** ‚Äî measures prediction error.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Summary\n",
    "\n",
    "| Component | Role |\n",
    "|------------|------|\n",
    "| **Server** | Coordinates training, aggregates client updates |\n",
    "| **Clients** | Train local models on private data |\n",
    "| **FedAvg** | Averages model weights across clients |\n",
    "| **Rounds** | Number of communication cycles between server & clients |\n",
    "\n",
    "---\n",
    "\n",
    "## üìö References\n",
    "\n",
    "- McMahan, B. et al. (2017). *Communication-Efficient Learning of Deep Networks from Decentralized Data*. AISTATS.  \n",
    "- Google AI Blog (2017). *Federated Learning: Collaborative Machine Learning without Centralized Training Data.*\n",
    "\n",
    "---\n",
    "\n",
    "üëâ **In this notebook**, we simulate Federated Learning using the **FedAvg algorithm** with multiple clients, each training on its own subset of the dataset (non-IID partitions).  \n",
    "This setup allows you to **understand, visualize, and experiment** with client heterogeneity, communication rounds, and model aggregation in FL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ How We Simulate Federated Learning (single-process simulation)\n",
    "\n",
    "In this notebook we **simulate** a federated learning (FL) experiment inside a single Python process.  \n",
    "This is an educational and reproducible setup that mimics the high-level behavior of a real FL system while keeping the code simple to run on a laptop.\n",
    "\n",
    "Below is a description of what the code does and how the simulation maps to real FL components.\n",
    "\n",
    "---\n",
    "\n",
    "## Simulation design (high level)\n",
    "\n",
    "1. **Create clients (nodes)**  \n",
    "   We simulate `K` clients by partitioning the dataset into `K` disjoint subsets (one per client).  \n",
    "   Partitioning can be IID or non-IID ‚Äî in this notebook we use a Dirichlet-based split to create **non-IID** client data.\n",
    "\n",
    "2. **Server initialises a global model**  \n",
    "   A single global model (e.g., a CNN) lives on the server and is copied to each client at the start of every round.\n",
    "\n",
    "3. **Federated rounds (main loop)**  \n",
    "   For each round:\n",
    "   - The server **sends** the current global weights to all clients (in code: `local_model.load_state_dict(global_model.state_dict())`).\n",
    "   - Each client trains the model **locally** on its own data for a few epochs and returns its updated model object.\n",
    "     - In our simulation, local training is done **serially** in a loop (client 0, client 1, ... client K-1).\n",
    "     - In real FL, clients usually train in parallel and communicate asynchronously or in rounds.\n",
    "   - The server **aggregates** the returned local model parameters using **FedAvg** (weighted average, typically by client sample count).\n",
    "   - The aggregated parameters replace the server's global model for the next round.\n",
    "\n",
    "4. **Logging & saving**  \n",
    "   - We record per-client training losses and validation metrics after local updates.\n",
    "   - We evaluate the **global model** on each client‚Äôs test set and save the best global `state_dict()` when it improves.\n",
    "\n",
    "5. **Final evaluation**  \n",
    "   - After training, we load and evaluate the saved **best local models** (if saved per-client) and the **final/global model** on every client‚Äôs test data to compare local vs global performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Key implementation details (mapping to code cells)\n",
    "\n",
    "- **Data partitioning**: `split_data_non_iid(...)` (in `loaders_federated_learning.py`) uses a Dirichlet distribution to simulate heterogeneity across clients.  \n",
    "- **Client DataLoaders**: `get_loaders(...)` (in `loaders_federated_learning.py`) returns a list `Loaders` where `Loaders[k] = [train_loader, val_loader, test_loader]` for client `k`.  \n",
    "- **Local training**: `train_model(...)` (in `Client.py`) performs training for a client and returns the updated `local_model` and training loss.  \n",
    "- **Aggregation**: `federated_averaging(models, weights)` computes weighted average of client parameters; server updates global model via `global_model.load_state_dict(...)`.  \n",
    "- **Evaluation**: `evaluate_models(...)` and `evaluate_models_test(...)` compute AUC / APR per client; `prediction_binary(...)` runs final test predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## Why serial simulation is OK for simulation\n",
    "- **Deterministic & simple** ‚Äî easy to debug and reproduce in a classroom or on a laptop.\n",
    "- **Focuses on algorithmic ideas** (FedAvg, non-IID effects, client heterogeneity) without the complexity of networking, concurrency, or device management.\n",
    "- **Easily extensible** to parallel/real FL frameworks later (e.g., Flower, TensorFlow Federated, PySyft).\n",
    "\n",
    "---\n",
    "\n",
    "## Limitations vs real FL systems (be aware)\n",
    "- **No network conditions**: We ignore latency, bandwidth, and dropped clients.\n",
    "- **No client asynchrony**: Clients are simulated serially; we don‚Äôt model stragglers or partial participation unless explicitly coded.\n",
    "- **Single process memory**: All model copies and data live in the same process and memory; not realistic for large-scale deployments.\n",
    "- **Privacy guarantees**: Simulation alone does not provide privacy (e.g., differential privacy or secure aggregation must be added explicitly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß© Exercises ‚Äî Explore and Extend the Federated Learning Framework\n",
    "\n",
    "Now that weve explored how federated learning (FL) is simulated in this notebook, it‚Äôs time to apply your understanding.  \n",
    "Complete the following exercises to strengthen your conceptual and practical grasp of FL.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Exercise 1 ‚Äî Implement the Federated Averaging Function\n",
    "\n",
    "**Objective:**  \n",
    "Implement the `federated_averaging(models, weights=None)` function to aggregate local client models into a single global model.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Extract parameters (or the `state_dict`) from each client model.\n",
    "2. Compute a **weighted average** of all client model parameters.\n",
    "3. If no `weights` are provided, assume **equal weighting** for all clients.\n",
    "4. Return the averaged parameters as a new `state_dict` that can be loaded directly into the global model:\n",
    "\n",
    "\n",
    "## üß† Exercise 2 ‚Äî Using LSTM-based classifier\n",
    "\n",
    "**Objective:**  \n",
    "Replace `CNNClassifier` with `LSTMCLassifier` and analyse the difference in performance.\n",
    "\n",
    "**Instructions:**\n",
    "Replace both global and local models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7013da73a6d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# Imports, utilities, and experiment setup\n",
    "# -----------------------------------------\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Local modules (models, data loader, client training and utils)\n",
    "# - Models: contains model classes (e.g. LSTMClassifier, CNNClassifier)\n",
    "# - loaders_federated_learning: data splitting and per-client DataLoader creation\n",
    "# - Client: per-client training/evaluation logic for federated simulation\n",
    "# - utils: metrics, logging helpers, etc.\n",
    "\n",
    "from Models import LSTMClassifier, CNNClassifier   # In this excersice, we will use 1-D CNN model for time-series classification.\n",
    "from loaders_federated_learning import get_loaders\n",
    "from Client import *\n",
    "from utils import *\n",
    "\n",
    "# -------------------------\n",
    "# Warnings & display setup\n",
    "# -------------------------\n",
    "warnings.filterwarnings(\"ignore\")  # hide noisy warnings (useful for notebook runs)\n",
    "\n",
    "# -------------------------\n",
    "# Deterministic seeds / RNG\n",
    "# -------------------------\n",
    "# Set seeds for reproducibility. Note: exact reproducibility across platforms/hardware\n",
    "# (especially with CUDA) may still vary; the flags below reduce nondeterminism.\n",
    "SEED = 20\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1/5 Summary:\n",
      "  Total samples: 1957\n",
      "  Training samples: 1126, Validation: 732, Test: 99\n",
      "  Positive class ratio: 0.440 (44.0%)\n",
      "-----------------------------------\n",
      "Client 2/5 Summary:\n",
      "  Total samples: 7329\n",
      "  Training samples: 5942, Validation: 973, Test: 414\n",
      "  Positive class ratio: 0.031 (3.1%)\n",
      "-----------------------------------\n",
      "Client 3/5 Summary:\n",
      "  Total samples: 1195\n",
      "  Training samples: 81, Validation: 863, Test: 251\n",
      "  Positive class ratio: 0.150 (15.0%)\n",
      "-----------------------------------\n",
      "Client 4/5 Summary:\n",
      "  Total samples: 5667\n",
      "  Training samples: 5078, Validation: 265, Test: 324\n",
      "  Positive class ratio: 0.173 (17.3%)\n",
      "-----------------------------------\n",
      "Client 5/5 Summary:\n",
      "  Total samples: 5308\n",
      "  Training samples: 2571, Validation: 489, Test: 2248\n",
      "  Positive class ratio: 0.133 (13.3%)\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Define number of clients (Nodes) and get dataloaders for each client\n",
    "# -------------------------\n",
    "nodes = 5 ## lets create 5 clients\n",
    "Loaders,weights=get_loaders(nodes=nodes)  ### weights is the percentage of training data assigned to each client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CNNClassifier(\n",
      "  (conv1): Conv1d(76, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (activation): Sigmoid()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Select device (CPU or GPU)\n",
    "# -------------------------\n",
    "# get_device() is implemented in utils and should return either:\n",
    "#   - torch.device(\"cuda\")  if a GPU is available, or\n",
    "#   - torch.device(\"cpu\")   otherwise\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------------\n",
    "# Define the global model (server-side)\n",
    "# -------------------------\n",
    "# Infer number of input features from the first client's training dataset.\n",
    "# Assumes TensorDataset where each sample is (features, label) and features shape is (seq_len, n_features)\n",
    "n_features = Loaders[0][0].dataset[0][0].shape[1]\n",
    "\n",
    "num_filters = 64              # number of convolutional filters in the CNN\n",
    "global_model = CNNClassifier(n_features, num_filters, device)\n",
    "global_model.to(device)       # move model to chosen device\n",
    "print(global_model)\n",
    "# -------------------------\n",
    "# Loss function\n",
    "# -------------------------\n",
    "criterion = nn.BCELoss().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is just to create the performance logs in each client.\n",
    "\n",
    "DF = [0]*nodes\n",
    "# List for best val auc at each client\n",
    "Val_AUC = [0]*nodes\n",
    "Val_APR = [0]*nodes\n",
    "\n",
    "for h in range(0, nodes):\n",
    "    DF[h] = pd.DataFrame(columns=['Train_Loss', 'Val_Loss', 'Val_AUC','Val_APR'])  ### Dataframe for each client to document training and validation performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------------------------------------------------------------\n",
    "### Federated Averaging (FedAvg)\n",
    "### ------------------------------------------------------------\n",
    "### Combines multiple client models into a single global model\n",
    "### by averaging their parameters (weighted or uniform).\n",
    "###\n",
    "### Args:\n",
    "###   models  : list of PyTorch model objects (each client‚Äôs local model)\n",
    "###   weights : list or array of weights (one per client), optional.\n",
    "###             If None ‚Üí uniform averaging (equal contribution).\n",
    "###\n",
    "### Returns:\n",
    "###   averaged_params : list of averaged parameter tensors\n",
    "###                     (same order as model.state_dict().values())\n",
    "### ------------------------------------------------------------\n",
    "\n",
    "def federated_averaging(models, weights=None, device=None):\n",
    "    \"\"\"\n",
    "    Average client models' state_dicts (FedAvg) and return an averaged state_dict.\n",
    "\n",
    "    Args:\n",
    "        models (list): list of PyTorch model objects (all same architecture)\n",
    "        weights (list or None): optional list of weights (one per client). If None, uniform weights used.\n",
    "        device (torch.device or str or None): device where resulting tensors should be placed.\n",
    "                                             If None, uses CPU for the averaged tensors.\n",
    "    Returns:\n",
    "        avg_state_dict (dict): averaged state_dict (ready for global_model.load_state_dict(avg_state_dict))\n",
    "    \"\"\"\n",
    "    if len(models) == 0:\n",
    "        raise ValueError(\"No models provided to federated_averaging.\")\n",
    "\n",
    "    # 1) Collect state_dicts\n",
    "    state_dicts = [m.state_dict() for m in models]\n",
    "    keys = list(state_dicts[0].keys())\n",
    "\n",
    "    # 2) Validate that all models have the same keys / shapes\n",
    "    for sd in state_dicts[1:]:\n",
    "        if sd.keys() != state_dicts[0].keys():\n",
    "            raise ValueError(\"All models must have the same state_dict keys/architecture.\")\n",
    "\n",
    "    num_clients = len(state_dicts)\n",
    "\n",
    "    # 3) Prepare/normalize weights\n",
    "    if weights is None:\n",
    "        weights = [1.0 / num_clients] * num_clients\n",
    "    else:\n",
    "        total = float(sum(weights))\n",
    "        if total == 0.0:\n",
    "            raise ValueError(\"Sum of weights must be > 0.\")\n",
    "        weights = [float(w) / total for w in weights]\n",
    "\n",
    "    # 4) Decide device for averaged params\n",
    "    if device is None:\n",
    "        device = torch.device(\"cpu\")\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    # 5) Build averaged state dict\n",
    "    avg_state = {}\n",
    "    for key in keys:\n",
    "        # initialize accumulator with zeros on correct device & dtype\n",
    "        accum = torch.zeros_like(state_dicts[0][key], device=device, dtype=state_dicts[0][key].dtype)\n",
    "        for sd, w in zip(state_dicts, weights):\n",
    "            # move client tensor to accumulation device and multiply by weight\n",
    "            client_tensor = sd[key].to(device)\n",
    "            accum += client_tensor * w\n",
    "        avg_state[key] = accum\n",
    "\n",
    "    return avg_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ Round 1/15 ================\n",
      "\n",
      "--> Training on Client 1/5\n",
      "--> Training on Client 2/5\n",
      "--> Training on Client 3/5\n",
      "--> Training on Client 4/5\n",
      "--> Training on Client 5/5\n",
      "Client 01 | Train Loss: 0.532 | Best Val AUC: 0.814 | Current AUC: 0.814 | Current APR: 0.614\n",
      "Client 02 | Train Loss: 0.050 | Best Val AUC: 0.754 | Current AUC: 0.754 | Current APR: 0.110\n",
      "Client 03 | Train Loss: 0.444 | Best Val AUC: 0.766 | Current AUC: 0.766 | Current APR: 0.101\n",
      "Client 04 | Train Loss: 0.328 | Best Val AUC: 0.828 | Current AUC: 0.828 | Current APR: 0.843\n",
      "Client 05 | Train Loss: 0.412 | Best Val AUC: 0.792 | Current AUC: 0.792 | Current APR: 0.491\n",
      "------------------------------------------------------------\n",
      "\n",
      ">>> Global Model Average AUC (Round 1): 0.8126\n",
      "============================================================\n",
      "\n",
      "\n",
      "================ Round 2/15 ================\n",
      "\n",
      "--> Training on Client 1/5\n",
      "--> Training on Client 2/5\n",
      "--> Training on Client 3/5\n",
      "--> Training on Client 4/5\n",
      "--> Training on Client 5/5\n",
      "Client 01 | Train Loss: 0.518 | Best Val AUC: 0.822 | Current AUC: 0.822 | Current APR: 0.634\n",
      "Client 02 | Train Loss: 0.048 | Best Val AUC: 0.754 | Current AUC: 0.733 | Current APR: 0.087\n",
      "Client 03 | Train Loss: 0.243 | Best Val AUC: 0.818 | Current AUC: 0.818 | Current APR: 0.136\n",
      "Client 04 | Train Loss: 0.310 | Best Val AUC: 0.843 | Current AUC: 0.843 | Current APR: 0.864\n",
      "Client 05 | Train Loss: 0.390 | Best Val AUC: 0.805 | Current AUC: 0.805 | Current APR: 0.499\n",
      "------------------------------------------------------------\n",
      "\n",
      ">>> Global Model Average AUC (Round 2): 0.8170\n",
      "============================================================\n",
      "\n",
      "\n",
      "================ Round 3/15 ================\n",
      "\n",
      "--> Training on Client 1/5\n",
      "--> Training on Client 2/5\n",
      "--> Training on Client 3/5\n",
      "--> Training on Client 4/5\n",
      "--> Training on Client 5/5\n",
      "Client 01 | Train Loss: 0.507 | Best Val AUC: 0.829 | Current AUC: 0.829 | Current APR: 0.656\n",
      "Client 02 | Train Loss: 0.045 | Best Val AUC: 0.754 | Current AUC: 0.713 | Current APR: 0.077\n",
      "Client 03 | Train Loss: 0.300 | Best Val AUC: 0.825 | Current AUC: 0.825 | Current APR: 0.175\n",
      "Client 04 | Train Loss: 0.306 | Best Val AUC: 0.843 | Current AUC: 0.838 | Current APR: 0.859\n",
      "Client 05 | Train Loss: 0.386 | Best Val AUC: 0.813 | Current AUC: 0.813 | Current APR: 0.526\n",
      "------------------------------------------------------------\n",
      "\n",
      ">>> Global Model Average AUC (Round 3): 0.8217\n",
      "============================================================\n",
      "\n",
      "\n",
      "================ Round 4/15 ================\n",
      "\n",
      "--> Training on Client 1/5\n",
      "--> Training on Client 2/5\n",
      "--> Training on Client 3/5\n",
      "--> Training on Client 4/5\n",
      "--> Training on Client 5/5\n",
      "Client 01 | Train Loss: 0.507 | Best Val AUC: 0.831 | Current AUC: 0.831 | Current APR: 0.661\n",
      "Client 02 | Train Loss: 0.041 | Best Val AUC: 0.754 | Current AUC: 0.672 | Current APR: 0.071\n",
      "Client 03 | Train Loss: 0.214 | Best Val AUC: 0.833 | Current AUC: 0.833 | Current APR: 0.221\n",
      "Client 04 | Train Loss: 0.296 | Best Val AUC: 0.843 | Current AUC: 0.843 | Current APR: 0.862\n",
      "Client 05 | Train Loss: 0.378 | Best Val AUC: 0.817 | Current AUC: 0.817 | Current APR: 0.519\n",
      "------------------------------------------------------------\n",
      "\n",
      ">>> Global Model Average AUC (Round 4): 0.8235\n",
      "============================================================\n",
      "\n",
      "\n",
      "================ Round 5/15 ================\n",
      "\n",
      "--> Training on Client 1/5\n",
      "--> Training on Client 2/5\n",
      "--> Training on Client 3/5\n",
      "--> Training on Client 4/5\n",
      "--> Training on Client 5/5\n",
      "Client 01 | Train Loss: 0.526 | Best Val AUC: 0.831 | Current AUC: 0.828 | Current APR: 0.667\n",
      "Client 02 | Train Loss: 0.039 | Best Val AUC: 0.754 | Current AUC: 0.661 | Current APR: 0.073\n",
      "Client 03 | Train Loss: 0.213 | Best Val AUC: 0.833 | Current AUC: 0.818 | Current APR: 0.226\n",
      "Client 04 | Train Loss: 0.298 | Best Val AUC: 0.843 | Current AUC: 0.833 | Current APR: 0.861\n",
      "Client 05 | Train Loss: 0.385 | Best Val AUC: 0.821 | Current AUC: 0.821 | Current APR: 0.531\n",
      "------------------------------------------------------------\n",
      "\n",
      ">>> Global Model Average AUC (Round 5): 0.8175\n",
      "============================================================\n",
      "\n",
      "\n",
      "================ Round 6/15 ================\n",
      "\n",
      "--> Training on Client 1/5\n",
      "--> Training on Client 2/5\n",
      "--> Training on Client 3/5\n",
      "--> Training on Client 4/5\n",
      "--> Training on Client 5/5\n",
      "Client 01 | Train Loss: 0.512 | Best Val AUC: 0.831 | Current AUC: 0.825 | Current APR: 0.669\n",
      "Client 02 | Train Loss: 0.039 | Best Val AUC: 0.754 | Current AUC: 0.654 | Current APR: 0.075\n",
      "Client 03 | Train Loss: 0.173 | Best Val AUC: 0.833 | Current AUC: 0.823 | Current APR: 0.253\n",
      "Client 04 | Train Loss: 0.289 | Best Val AUC: 0.843 | Current AUC: 0.833 | Current APR: 0.855\n",
      "Client 05 | Train Loss: 0.364 | Best Val AUC: 0.821 | Current AUC: 0.815 | Current APR: 0.517\n",
      "------------------------------------------------------------\n",
      "\n",
      ">>> Global Model Average AUC (Round 6): 0.8151\n",
      "============================================================\n",
      "\n",
      "\n",
      "================ Round 7/15 ================\n",
      "\n",
      "--> Training on Client 1/5\n",
      "--> Training on Client 2/5\n",
      "--> Training on Client 3/5\n",
      "--> Training on Client 4/5\n",
      "--> Training on Client 5/5\n",
      "Client 01 | Train Loss: 0.535 | Best Val AUC: 0.831 | Current AUC: 0.822 | Current APR: 0.662\n",
      "Client 02 | Train Loss: 0.035 | Best Val AUC: 0.754 | Current AUC: 0.640 | Current APR: 0.086\n",
      "Client 03 | Train Loss: 0.173 | Best Val AUC: 0.833 | Current AUC: 0.820 | Current APR: 0.255\n",
      "Client 04 | Train Loss: 0.284 | Best Val AUC: 0.843 | Current AUC: 0.835 | Current APR: 0.857\n",
      "Client 05 | Train Loss: 0.366 | Best Val AUC: 0.827 | Current AUC: 0.827 | Current APR: 0.542\n",
      "------------------------------------------------------------\n",
      "\n",
      ">>> Global Model Average AUC (Round 7): 0.8064\n",
      "============================================================\n",
      "\n",
      "\n",
      "================ Round 8/15 ================\n",
      "\n",
      "--> Training on Client 1/5\n",
      "--> Training on Client 2/5\n",
      "--> Training on Client 3/5\n",
      "--> Training on Client 4/5\n",
      "--> Training on Client 5/5\n",
      "Client 01 | Train Loss: 0.539 | Best Val AUC: 0.831 | Current AUC: 0.823 | Current APR: 0.673\n",
      "Client 02 | Train Loss: 0.033 | Best Val AUC: 0.754 | Current AUC: 0.617 | Current APR: 0.075\n",
      "Client 03 | Train Loss: 0.230 | Best Val AUC: 0.833 | Current AUC: 0.808 | Current APR: 0.200\n",
      "Client 04 | Train Loss: 0.284 | Best Val AUC: 0.843 | Current AUC: 0.838 | Current APR: 0.862\n",
      "Client 05 | Train Loss: 0.372 | Best Val AUC: 0.827 | Current AUC: 0.818 | Current APR: 0.546\n",
      "------------------------------------------------------------\n",
      "\n",
      ">>> Global Model Average AUC (Round 8): 0.8026\n",
      "============================================================\n",
      "\n",
      "\n",
      "================ Round 9/15 ================\n",
      "\n",
      "--> Training on Client 1/5\n",
      "--> Training on Client 2/5\n",
      "--> Training on Client 3/5\n",
      "--> Training on Client 4/5\n",
      "--> Training on Client 5/5\n",
      "Client 01 | Train Loss: 0.556 | Best Val AUC: 0.831 | Current AUC: 0.822 | Current APR: 0.672\n",
      "Client 02 | Train Loss: 0.031 | Best Val AUC: 0.754 | Current AUC: 0.616 | Current APR: 0.098\n",
      "Client 03 | Train Loss: 0.158 | Best Val AUC: 0.833 | Current AUC: 0.809 | Current APR: 0.221\n",
      "Client 04 | Train Loss: 0.272 | Best Val AUC: 0.843 | Current AUC: 0.826 | Current APR: 0.854\n",
      "Client 05 | Train Loss: 0.359 | Best Val AUC: 0.827 | Current AUC: 0.821 | Current APR: 0.533\n",
      "------------------------------------------------------------\n",
      "\n",
      ">>> Global Model Average AUC (Round 9): 0.7946\n",
      "============================================================\n",
      "\n",
      "\n",
      "================ Round 10/15 ================\n",
      "\n",
      "--> Training on Client 1/5\n",
      "--> Training on Client 2/5\n",
      "--> Training on Client 3/5\n",
      "--> Training on Client 4/5\n",
      "--> Training on Client 5/5\n",
      "Client 01 | Train Loss: 0.493 | Best Val AUC: 0.831 | Current AUC: 0.818 | Current APR: 0.665\n",
      "Client 02 | Train Loss: 0.029 | Best Val AUC: 0.754 | Current AUC: 0.607 | Current APR: 0.089\n",
      "Client 03 | Train Loss: 0.213 | Best Val AUC: 0.833 | Current AUC: 0.808 | Current APR: 0.181\n",
      "Client 04 | Train Loss: 0.271 | Best Val AUC: 0.843 | Current AUC: 0.836 | Current APR: 0.861\n",
      "Client 05 | Train Loss: 0.358 | Best Val AUC: 0.827 | Current AUC: 0.809 | Current APR: 0.512\n",
      "------------------------------------------------------------\n",
      "\n",
      ">>> Global Model Average AUC (Round 10): 0.7937\n",
      "============================================================\n",
      "\n",
      "\n",
      "================ Round 11/15 ================\n",
      "\n",
      "--> Training on Client 1/5\n",
      "--> Training on Client 2/5\n",
      "--> Training on Client 3/5\n",
      "--> Training on Client 4/5\n",
      "--> Training on Client 5/5\n",
      "Client 01 | Train Loss: 0.494 | Best Val AUC: 0.831 | Current AUC: 0.814 | Current APR: 0.664\n",
      "Client 02 | Train Loss: 0.025 | Best Val AUC: 0.754 | Current AUC: 0.603 | Current APR: 0.080\n",
      "Client 03 | Train Loss: 0.193 | Best Val AUC: 0.833 | Current AUC: 0.807 | Current APR: 0.168\n",
      "Client 04 | Train Loss: 0.261 | Best Val AUC: 0.843 | Current AUC: 0.825 | Current APR: 0.848\n",
      "Client 05 | Train Loss: 0.356 | Best Val AUC: 0.827 | Current AUC: 0.813 | Current APR: 0.510\n",
      "------------------------------------------------------------\n",
      "\n",
      ">>> Global Model Average AUC (Round 11): 0.7909\n",
      "============================================================\n",
      "\n",
      "\n",
      "================ Round 12/15 ================\n",
      "\n",
      "--> Training on Client 1/5\n",
      "--> Training on Client 2/5\n",
      "--> Training on Client 3/5\n",
      "--> Training on Client 4/5\n",
      "--> Training on Client 5/5\n",
      "Client 01 | Train Loss: 0.487 | Best Val AUC: 0.831 | Current AUC: 0.811 | Current APR: 0.660\n",
      "Client 02 | Train Loss: 0.023 | Best Val AUC: 0.754 | Current AUC: 0.621 | Current APR: 0.102\n",
      "Client 03 | Train Loss: 0.151 | Best Val AUC: 0.833 | Current AUC: 0.799 | Current APR: 0.160\n",
      "Client 04 | Train Loss: 0.255 | Best Val AUC: 0.843 | Current AUC: 0.825 | Current APR: 0.851\n",
      "Client 05 | Train Loss: 0.346 | Best Val AUC: 0.827 | Current AUC: 0.801 | Current APR: 0.525\n",
      "------------------------------------------------------------\n",
      "\n",
      ">>> Global Model Average AUC (Round 12): 0.7926\n",
      "============================================================\n",
      "\n",
      "\n",
      "================ Round 13/15 ================\n",
      "\n",
      "--> Training on Client 1/5\n",
      "--> Training on Client 2/5\n",
      "--> Training on Client 3/5\n",
      "--> Training on Client 4/5\n",
      "--> Training on Client 5/5\n",
      "Client 01 | Train Loss: 0.504 | Best Val AUC: 0.831 | Current AUC: 0.807 | Current APR: 0.655\n",
      "Client 02 | Train Loss: 0.022 | Best Val AUC: 0.754 | Current AUC: 0.604 | Current APR: 0.085\n",
      "Client 03 | Train Loss: 0.279 | Best Val AUC: 0.833 | Current AUC: 0.799 | Current APR: 0.169\n",
      "Client 04 | Train Loss: 0.246 | Best Val AUC: 0.843 | Current AUC: 0.829 | Current APR: 0.849\n",
      "Client 05 | Train Loss: 0.347 | Best Val AUC: 0.827 | Current AUC: 0.807 | Current APR: 0.541\n",
      "------------------------------------------------------------\n",
      "\n",
      ">>> Global Model Average AUC (Round 13): 0.7840\n",
      "============================================================\n",
      "\n",
      "\n",
      "================ Round 14/15 ================\n",
      "\n",
      "--> Training on Client 1/5\n",
      "--> Training on Client 2/5\n",
      "--> Training on Client 3/5\n",
      "--> Training on Client 4/5\n",
      "--> Training on Client 5/5\n",
      "Client 01 | Train Loss: 0.467 | Best Val AUC: 0.831 | Current AUC: 0.810 | Current APR: 0.659\n",
      "Client 02 | Train Loss: 0.021 | Best Val AUC: 0.754 | Current AUC: 0.627 | Current APR: 0.103\n",
      "Client 03 | Train Loss: 0.237 | Best Val AUC: 0.833 | Current AUC: 0.794 | Current APR: 0.150\n",
      "Client 04 | Train Loss: 0.238 | Best Val AUC: 0.843 | Current AUC: 0.810 | Current APR: 0.842\n",
      "Client 05 | Train Loss: 0.339 | Best Val AUC: 0.827 | Current AUC: 0.801 | Current APR: 0.500\n",
      "------------------------------------------------------------\n",
      "\n",
      ">>> Global Model Average AUC (Round 14): 0.7729\n",
      "============================================================\n",
      "\n",
      "\n",
      "================ Round 15/15 ================\n",
      "\n",
      "--> Training on Client 1/5\n",
      "--> Training on Client 2/5\n",
      "--> Training on Client 3/5\n",
      "--> Training on Client 4/5\n",
      "--> Training on Client 5/5\n",
      "Client 01 | Train Loss: 0.439 | Best Val AUC: 0.831 | Current AUC: 0.809 | Current APR: 0.661\n",
      "Client 02 | Train Loss: 0.018 | Best Val AUC: 0.754 | Current AUC: 0.603 | Current APR: 0.077\n",
      "Client 03 | Train Loss: 0.263 | Best Val AUC: 0.833 | Current AUC: 0.778 | Current APR: 0.130\n",
      "Client 04 | Train Loss: 0.234 | Best Val AUC: 0.843 | Current AUC: 0.809 | Current APR: 0.847\n",
      "Client 05 | Train Loss: 0.328 | Best Val AUC: 0.827 | Current AUC: 0.797 | Current APR: 0.491\n",
      "------------------------------------------------------------\n",
      "\n",
      ">>> Global Model Average AUC (Round 15): 0.7656\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Main Federated Training Loop (Server Orchestration)\n",
    "# ------------------------------------------------------------\n",
    "# Each round represents one complete cycle of:\n",
    "#   1. Server sending the global model to all clients\n",
    "#   2. Each client training locally on its own data\n",
    "#   3. Clients returning their updated local models\n",
    "#   4. Server aggregating the local updates (FedAvg)\n",
    "# ============================================================\n",
    "\n",
    "num_rounds = 15    # total federated communication rounds\n",
    "best = 0           # track the best global AUROC during training\n",
    "\n",
    "for round_num in range(num_rounds):\n",
    "    print(f\"\\n================ Round {round_num + 1}/{num_rounds} ================\\n\")\n",
    "\n",
    "    client_samples = []   # store each client‚Äôs locally trained model\n",
    "    LOSS = []             # store local training losses for logging\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Client-side training (simulated serially here for simplicity)\n",
    "    # In real FL, this happens in parallel on different devices\n",
    "    # ------------------------------------------------------------\n",
    "    for client_id in range(nodes):\n",
    "        print(f\"--> Training on Client {client_id + 1}/{nodes}\")\n",
    "\n",
    "        # Initialize a fresh local model and load current global weights\n",
    "        local_model = CNNClassifier(n_features, num_filters, device).to(device)\n",
    "        local_model.load_state_dict(global_model.state_dict())  # sync with server\n",
    "\n",
    "        # Define optimizer and perform local training on this client‚Äôs data\n",
    "        optimizer = torch.optim.Adam(local_model.parameters(), lr=0.001)\n",
    "        local_model, loss = train_model(\n",
    "            local_model,\n",
    "            Loaders[client_id][0],   # train loader for this client\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            device=device,\n",
    "            num_epochs=5\n",
    "        )  ## this function is from Clients.py\n",
    "        LOSS.append(loss)\n",
    "        client_samples.append(local_model)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Server-side aggregation (FedAvg)\n",
    "    # ------------------------------------------------------------\n",
    "    # aggregated_params = federated_averaging(client_samples, weights) ### if you dont pass weights, each client will be given equal importance in the aggregation\n",
    "\n",
    "    # # Update the global model‚Äôs state dictionary with averaged parameters\n",
    "    # new_state_dict = {k: aggregated_params[i] for i, k in enumerate(global_model.state_dict().keys())}\n",
    "    # global_model.load_state_dict(new_state_dict)\n",
    "    # aggregated_state is a dict (state_dict-like) returned by federated_averaging\n",
    "    aggregated_state = federated_averaging(client_samples, weights=weights, device=device)\n",
    "    global_model.load_state_dict(aggregated_state)\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Validate local models (post-training performance on validation) and store best performing one at each client\n",
    "    # ------------------------------------------------------------\n",
    "    for k in range(nodes):\n",
    "        local_model = client_samples[k]\n",
    "        DF[k], Val_AUC[k], cur_auc, cur_apr = evaluate_models(\n",
    "            k, Loaders, local_model, criterion, device,\n",
    "            DF[k], Val_AUC, LOSS[k], 'FedAvg'\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Client {k + 1:02d} | \"\n",
    "            f\"Train Loss: {LOSS[k]:.3f} | \"\n",
    "            f\"Best Val AUC: {Val_AUC[k]:.3f} | \"\n",
    "            f\"Current AUC: {cur_auc:.3f} | \"\n",
    "            f\"Current APR: {cur_apr:.3f}\"\n",
    "        )\n",
    "\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Validate the updated global model on each client‚Äôs test set\n",
    "    # ------------------------------------------------------------\n",
    "    total_auc = 0\n",
    "    for k in range(nodes):\n",
    "        _, cur_auc, cur_apr = evaluate_models_test(k, Loaders, global_model, criterion, device)\n",
    "        total_auc += cur_auc\n",
    "\n",
    "    global_auc = total_auc / nodes\n",
    "    print(f\"\\n>>> Global Model Average AUC (Round {round_num + 1}): {global_auc:.4f}\")\n",
    "\n",
    "    # Save the global model if it achieves a new best AUC\n",
    "    if global_auc > best:\n",
    "        best = global_auc\n",
    "        torch.save(global_model.state_dict(), './trained_models/FedAvg/global_model_state.pt')\n",
    "    print(\"============================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Evaluating Best Local Models ==========\n",
      "\n",
      "Client 01 | Test AUC: 0.8066 | Test APR: 0.3822\n",
      "------------------------------------------------------------\n",
      "Client 02 | Test AUC: 0.7651 | Test APR: 0.6094\n",
      "------------------------------------------------------------\n",
      "Client 03 | Test AUC: 0.7867 | Test APR: 0.8170\n",
      "------------------------------------------------------------\n",
      "Client 04 | Test AUC: 0.8284 | Test APR: 0.5197\n",
      "------------------------------------------------------------\n",
      "Client 05 | Test AUC: 0.8366 | Test APR: 0.1994\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üîç Evaluation at Each Client\n",
    "# ------------------------------------------------------------\n",
    "# Using the best-performing local models stored during\n",
    "# federated training (FedAvg).\n",
    "# ------------------------------------------------------------\n",
    "# For each client:\n",
    "#   1. Load the locally saved model\n",
    "#   2. Evaluate on that client's test set\n",
    "#   3. Record metrics (AUC, APR)\n",
    "#   4. Save results to CSV\n",
    "# ------------------------------------------------------------\n",
    "# Finally, compute and display average metrics across clients.\n",
    "# ============================================================\n",
    "\n",
    "from utils import *\n",
    "\n",
    "total_auc = 0.0\n",
    "total_apr = 0.0\n",
    "\n",
    "print(\"\\n========== Evaluating Best Local Models ==========\\n\")\n",
    "\n",
    "for client_id in range(nodes):\n",
    "    model_path = f'./trained_models/FedAvg/node{client_id}'\n",
    "    result_path = f'./Results/FedAvg/node{client_id}.csv'\n",
    "\n",
    "    # Load best local model for this client\n",
    "    local_model = torch.load(model_path, map_location=device)\n",
    "    local_model.to(device)\n",
    "    local_model.eval()\n",
    "\n",
    "    # Evaluate on client's test data (index 2 = test loader)\n",
    "    test_loss, test_auc, test_apr = prediction_binary(local_model, Loaders[client_id][2], criterion, device)\n",
    "\n",
    "    total_auc += test_auc\n",
    "    total_apr += test_apr\n",
    "\n",
    "    print(f\"Client {client_id + 1:02d} | \"\n",
    "          f\"Test AUC: {test_auc:.4f} | \"\n",
    "          f\"Test APR: {test_apr:.4f}\")\n",
    "\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Compute and display overall performance\n",
    "# --------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Model_IHM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m sum_apr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Load the best global model saved during training\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m global_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./trained_models/FedAvg/global_model\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     20\u001b[0m global_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m global_model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/serialization.py:1360\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1358\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1361\u001b[0m             opened_zipfile,\n\u001b[1;32m   1362\u001b[0m             map_location,\n\u001b[1;32m   1363\u001b[0m             pickle_module,\n\u001b[1;32m   1364\u001b[0m             overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[1;32m   1365\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1366\u001b[0m         )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1368\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1851\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/serialization.py:1837\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1835\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1836\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfind_class(mod_name, name)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Model_IHM'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üåç Evaluate the Final Global Model (FedAvg)\n",
    "# ------------------------------------------------------------\n",
    "# The saved global model is evaluated on each client‚Äôs test set\n",
    "# to assess generalization across distributed data.\n",
    "# ------------------------------------------------------------\n",
    "# For each client:\n",
    "#   1. Load the final global model\n",
    "#   2. Evaluate on the client's local test data\n",
    "#   3. Collect AUC and APR metrics\n",
    "# ------------------------------------------------------------\n",
    "# Finally, compute the mean AUC and APR across all clients.\n",
    "# ============================================================\n",
    "\n",
    "sum_auc = 0.0\n",
    "sum_apr = 0.0\n",
    "\n",
    "# Load the best global model saved during training\n",
    "# recreate the model architecture, then load state_dict\n",
    "model = CNNClassifier(n_features, num_filters, device)   # or the correct class\n",
    "state = torch.load('./trained_models/FedAvg/global_model_state.pt', map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# print(\"\\n========== Evaluating Final Global Model on All Clients ==========\\n\")\n",
    "\n",
    "# for client_id in range(nodes):\n",
    "#     # Evaluate global model on each client's test set\n",
    "#     test_loss, test_auc, test_apr = prediction_binary(global_model, Loaders[client_id][2], criterion, device)\n",
    "\n",
    "#     sum_auc += test_auc\n",
    "#     sum_apr += test_apr\n",
    "\n",
    "#     print(f\"Client {client_id + 1:02d} | \"\n",
    "#           f\"Test AUC: {test_auc:.4f} | \"\n",
    "#           f\"Test APR: {test_apr:.4f}\")\n",
    "#     print(\"------------------------------------------------------------\")\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # Compute and print global performance across all clients\n",
    "# # ------------------------------------------------------------\n",
    "# avg_auc = sum_auc / nodes\n",
    "# avg_apr = sum_apr / nodes\n",
    "\n",
    "# print(\"\\n================ Global Model Summary ==================\")\n",
    "# print(f\"Average Test AUC across {nodes} clients: {avg_auc:.4f}\")\n",
    "# print(f\"Average Test APR across {nodes} clients: {avg_apr:.4f}\")\n",
    "# print(\"========================================================\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
