{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Federated Learning?\n",
    "\n",
    "Federated Learning (FL) is a **decentralized machine learning approach** where multiple clients (e.g., hospitals, mobile devices, or institutions) collaboratively train a shared global model **without sharing their raw data**.\n",
    "\n",
    "Instead of pooling all data in a central server, each client:\n",
    "1. **Trains locally** on its own private dataset.\n",
    "2. **Sends model updates (parameters or gradients)** ‚Äî not the data ‚Äî to a central server.\n",
    "3. The **server aggregates** these updates to improve the **global model**.\n",
    "4. This process repeats over several **communication rounds** until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Why Federated Learning?\n",
    "\n",
    "Traditional centralized ML requires moving all data to one place ‚Äî often impractical or illegal due to **privacy**, **security**, or **data ownership** concerns.\n",
    "\n",
    "Federated Learning enables:\n",
    "- ‚úÖ **Data privacy** ‚Äî raw data never leaves the client.\n",
    "- ‚úÖ **Scalability** ‚Äî computation distributed across clients.\n",
    "- ‚úÖ **Collaboration** ‚Äî multiple institutions can contribute without sharing sensitive data.\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Federated Learning Workflow\n",
    "\n",
    "**1Ô∏è‚É£ Initialization (Server)**\n",
    "- The server defines a **global model** (e.g., CNN, LSTM).\n",
    "- Sends the model weights to all clients.\n",
    "\n",
    "**2Ô∏è‚É£ Local Training (Clients)**\n",
    "- Each client trains the model on its **local dataset** for a few epochs.\n",
    "- The resulting **updated model weights** are sent back to the server.\n",
    "\n",
    "**3Ô∏è‚É£ Aggregation (Server)**\n",
    "- The server combines all local updates (e.g., via **Federated Averaging ‚Äî FedAvg**).\n",
    "- Produces a new **global model** that captures knowledge from all clients.\n",
    "\n",
    "**4Ô∏è‚É£ Iteration**\n",
    "- The process repeats for multiple **federated rounds** until the global model converges.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Federated Averaging (FedAvg)\n",
    "\n",
    "FedAvg is the **core aggregation algorithm** in Federated Learning *(McMahan et al., 2017)*.  \n",
    "It computes the **weighted average of all client model parameters**:\n",
    "\n",
    "$$\n",
    "w_{t+1} = \\sum_{k=1}^{K} \\frac{n_k}{N} \\, w_{t+1}^{(k)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $w_{t+1}^{(k)}$: Model parameters from client *k*  \n",
    "- $n_k$: Number of training samples on client *k*  \n",
    "- $N = \\sum_k n_k$: Total number of samples across all clients  \n",
    "- $w_{t+1}$: Updated global model parameters  \n",
    "\n",
    "Clients with **more data** influence the global model **more**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è IID vs Non-IID Data\n",
    "\n",
    "In FL, client data distributions are often **non-IID** (not identically distributed):\n",
    "\n",
    "| Scenario | Description | Example |\n",
    "|-----------|--------------|----------|\n",
    "| **IID** | Clients have data drawn from similar distributions | Each hospital has balanced patient demographics |\n",
    "| **Non-IID** | Clients have biased or skewed data distributions | One hospital has mostly older patients, another mostly younger |\n",
    "\n",
    "Non-IID settings make FL more challenging since local models may diverge significantly ‚Äî proper aggregation and hyperparameter tuning are key.\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Evaluating FL Models\n",
    "\n",
    "After each round:\n",
    "- Each client‚Äôs **local model** can be evaluated on its **own validation/test set**.\n",
    "- The **global model** can also be evaluated on each client‚Äôs data to assess overall generalization.\n",
    "\n",
    "Common metrics:\n",
    "- **AUC (Area Under ROC Curve)** ‚Äî measures discrimination ability.\n",
    "- **APR (Average Precision Recall)** ‚Äî evaluates precision-recall trade-off.\n",
    "- **Loss (e.g., BCE, MSE)** ‚Äî measures prediction error.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Summary\n",
    "\n",
    "| Component | Role |\n",
    "|------------|------|\n",
    "| **Server** | Coordinates training, aggregates client updates |\n",
    "| **Clients** | Train local models on private data |\n",
    "| **FedAvg** | Averages model weights across clients |\n",
    "| **Rounds** | Number of communication cycles between server & clients |\n",
    "\n",
    "---\n",
    "\n",
    "## üìö References\n",
    "\n",
    "- McMahan, B. et al. (2017). *Communication-Efficient Learning of Deep Networks from Decentralized Data*. AISTATS.  \n",
    "- Google AI Blog (2017). *Federated Learning: Collaborative Machine Learning without Centralized Training Data.*\n",
    "\n",
    "---\n",
    "\n",
    "üëâ **In this notebook**, we simulate Federated Learning using the **FedAvg algorithm** with multiple clients, each training on its own subset of the dataset (non-IID partitions).  \n",
    "This setup allows you to **understand, visualize, and experiment** with client heterogeneity, communication rounds, and model aggregation in FL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ How We Simulate Federated Learning (single-process simulation)\n",
    "\n",
    "In this notebook we **simulate** a federated learning (FL) experiment inside a single Python process.  \n",
    "This is an educational and reproducible setup that mimics the high-level behavior of a real FL system while keeping the code simple to run on a laptop.\n",
    "\n",
    "Below is a description of what the code does and how the simulation maps to real FL components.\n",
    "\n",
    "---\n",
    "\n",
    "## Simulation design (high level)\n",
    "\n",
    "1. **Create clients (nodes)**  \n",
    "   We simulate `K` clients by partitioning the dataset into `K` disjoint subsets (one per client).  \n",
    "   Partitioning can be IID or non-IID ‚Äî in this notebook we use a Dirichlet-based split to create **non-IID** client data.\n",
    "\n",
    "2. **Server initialises a global model**  \n",
    "   A single global model (e.g., a CNN) lives on the server and is copied to each client at the start of every round.\n",
    "\n",
    "3. **Federated rounds (main loop)**  \n",
    "   For each round:\n",
    "   - The server **sends** the current global weights to all clients (in code: `local_model.load_state_dict(global_model.state_dict())`).\n",
    "   - Each client trains the model **locally** on its own data for a few epochs and returns its updated model object.\n",
    "     - In our simulation, local training is done **serially** in a loop (client 0, client 1, ... client K-1).\n",
    "     - In real FL, clients usually train in parallel and communicate asynchronously or in rounds.\n",
    "   - The server **aggregates** the returned local model parameters using **FedAvg** (weighted average, typically by client sample count).\n",
    "   - The aggregated parameters replace the server's global model for the next round.\n",
    "\n",
    "4. **Logging & saving**  \n",
    "   - We record per-client training losses and validation metrics after local updates.\n",
    "   - We evaluate the **global model** on each client‚Äôs test set and save the best global `state_dict()` when it improves.\n",
    "\n",
    "5. **Final evaluation**  \n",
    "   - After training, we load and evaluate the saved **best local models** (if saved per-client) and the **final/global model** on every client‚Äôs test data to compare local vs global performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Key implementation details (mapping to code cells)\n",
    "\n",
    "- **Data partitioning**: `split_data_non_iid(...)` (in `loaders_federated_learning.py`) uses a Dirichlet distribution to simulate heterogeneity across clients.  \n",
    "- **Client DataLoaders**: `get_loaders(...)` (in `loaders_federated_learning.py`) returns a list `Loaders` where `Loaders[k] = [train_loader, val_loader, test_loader]` for client `k`.  \n",
    "- **Local training**: `train_model(...)` (in `Client.py`) performs training for a client and returns the updated `local_model` and training loss.  \n",
    "- **Aggregation**: `federated_averaging(models, weights)` computes weighted average of client parameters; server updates global model via `global_model.load_state_dict(...)`.  \n",
    "- **Evaluation**: `evaluate_models(...)` and `evaluate_models_test(...)` compute AUC / APR per client; `prediction_binary(...)` runs final test predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## Why serial simulation is OK for simulation\n",
    "- **Deterministic & simple** ‚Äî easy to debug and reproduce in a classroom or on a laptop.\n",
    "- **Focuses on algorithmic ideas** (FedAvg, non-IID effects, client heterogeneity) without the complexity of networking, concurrency, or device management.\n",
    "- **Easily extensible** to parallel/real FL frameworks later (e.g., Flower, TensorFlow Federated, PySyft).\n",
    "\n",
    "---\n",
    "\n",
    "## Limitations vs real FL systems (be aware)\n",
    "- **No network conditions**: We ignore latency, bandwidth, and dropped clients.\n",
    "- **No client asynchrony**: Clients are simulated serially; we don‚Äôt model stragglers or partial participation unless explicitly coded.\n",
    "- **Single process memory**: All model copies and data live in the same process and memory; not realistic for large-scale deployments.\n",
    "- **Privacy guarantees**: Simulation alone does not provide privacy (e.g., differential privacy or secure aggregation must be added explicitly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß© Exercises ‚Äî Explore and Extend the Federated Learning Framework\n",
    "\n",
    "Now that weve explored how federated learning (FL) is simulated in this notebook, it‚Äôs time to apply your understanding.  \n",
    "Complete the following exercises to strengthen your conceptual and practical grasp of FL.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Exercise 1 ‚Äî Implement the Federated Averaging Function\n",
    "\n",
    "**Objective:**  \n",
    "Implement the `federated_averaging(models, weights=None)` function to aggregate local client models into a single global model.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Extract parameters (or the `state_dict`) from each client model.\n",
    "2. Compute a **weighted average** of all client model parameters.\n",
    "3. If no `weights` are provided, assume **equal weighting** for all clients.\n",
    "4. Return the averaged parameters as a new `state_dict` that can be loaded directly into the global model:\n",
    "\n",
    "\n",
    "## üß† Exercise 2 ‚Äî Using LSTM-based classifier\n",
    "\n",
    "**Objective:**  \n",
    "Replace `CNNClassifier` with `LSTMCLassifier` and analyse the difference in performance.\n",
    "\n",
    "**Instructions:**\n",
    "Replace both global and local models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Imports, utilities, and experiment setup\n",
    "# -----------------------------------------\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Local modules (models, data loader, client training and utils)\n",
    "# - Models: contains model classes (e.g. LSTMClassifier, CNNClassifier)\n",
    "# - loaders_federated_learning: data splitting and per-client DataLoader creation\n",
    "# - Client: per-client training/evaluation logic for federated simulation\n",
    "# - utils: metrics, logging helpers, etc.\n",
    "\n",
    "from Models import LSTMClassifier, CNNClassifier   # In this excersice, we will use 1-D CNN model for time-series classification.\n",
    "from loaders_federated_learning import get_loaders\n",
    "from Client import *\n",
    "from utils import *\n",
    "\n",
    "# -------------------------\n",
    "# Warnings & display setup\n",
    "# -------------------------\n",
    "warnings.filterwarnings(\"ignore\")  # hide noisy warnings (useful for notebook runs)\n",
    "\n",
    "# -------------------------\n",
    "# Deterministic seeds / RNG\n",
    "# -------------------------\n",
    "# Set seeds for reproducibility. Note: exact reproducibility across platforms/hardware\n",
    "# (especially with CUDA) may still vary; the flags below reduce nondeterminism.\n",
    "SEED = 20\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Define number of clients (Nodes) and get dataloaders for each client\n",
    "# -------------------------\n",
    "nodes = 5 ## lets create 5 clients\n",
    "Loaders,weights=get_loaders(nodes=nodes)  ### weights is the percentage of training data assigned to each client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Select device (CPU or GPU)\n",
    "# -------------------------\n",
    "# get_device() is implemented in utils and should return either:\n",
    "#   - torch.device(\"cuda\")  if a GPU is available, or\n",
    "#   - torch.device(\"cpu\")   otherwise\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------------\n",
    "# Define the global model (server-side)\n",
    "# -------------------------\n",
    "# Infer number of input features from the first client's training dataset.\n",
    "# Assumes TensorDataset where each sample is (features, label) and features shape is (seq_len, n_features)\n",
    "n_features = Loaders[0][0].dataset[0][0].shape[1]\n",
    "\n",
    "num_filters = 64              # number of convolutional filters in the CNN\n",
    "global_model = CNNClassifier(n_features, num_filters, device)\n",
    "global_model.to(device)       # move model to chosen device\n",
    "print(global_model)\n",
    "# -------------------------\n",
    "# Loss function\n",
    "# -------------------------\n",
    "criterion = nn.BCELoss().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is just to create the performance logs in each client.\n",
    "\n",
    "DF = [0]*nodes\n",
    "# List for best val auc at each client\n",
    "Val_AUC = [0]*nodes\n",
    "Val_APR = [0]*nodes\n",
    "\n",
    "for h in range(0, nodes):\n",
    "    DF[h] = pd.DataFrame(columns=['Train_Loss', 'Val_Loss', 'Val_AUC','Val_APR'])  ### Dataframe for each client to document training and validation performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üß© Federated Averaging (FedAvg)\n",
    "# ------------------------------------------------------------\n",
    "# Combines multiple client models into a single global model\n",
    "# by averaging their parameters (either weighted or uniform).\n",
    "#\n",
    "# Args:\n",
    "#   models  : list of PyTorch model objects (each client‚Äôs local model)\n",
    "#   weights : list or array of weights (one per client), optional.\n",
    "#             If None ‚Üí uniform weighting (equal contribution from all clients).\n",
    "#   device  : torch.device or string (e.g., 'cpu' or 'cuda') to specify\n",
    "#             where the averaged parameters will be stored.\n",
    "#\n",
    "# Returns:\n",
    "#   avg_state_dict : dictionary of averaged model parameters,\n",
    "#                    ready to be loaded into the global model via:\n",
    "#                    global_model.load_state_dict(avg_state_dict)\n",
    "#\n",
    "# ------------------------------------------------------------\n",
    "# ‚úèÔ∏è Student Task:\n",
    "# Implement the averaging logic below so that it correctly computes\n",
    "# the weighted average of model parameters from multiple clients.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def federated_averaging(models, weights=None, device=None):\n",
    "    \"\"\"\n",
    "    Average client models' state_dicts (FedAvg) and return an averaged state_dict.\n",
    "\n",
    "    Args:\n",
    "        models (list): List of PyTorch model objects (all with the same architecture)\n",
    "        weights (list or None): Optional list of weights (one per client). \n",
    "                                If None, assign equal weights to all clients.\n",
    "        device (torch.device or str or None): Device on which averaged tensors \n",
    "                                              should be placed (default: CPU).\n",
    "\n",
    "    Returns:\n",
    "        avg_state_dict (dict): Averaged model parameters, ready to be loaded into \n",
    "                               the global model via `load_state_dict()`.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(models) == 0:\n",
    "        raise ValueError(\"No models provided to federated_averaging().\")\n",
    "\n",
    "    # Extract state_dicts from all client models\n",
    "    state_dicts = [m.state_dict() for m in models]\n",
    "    keys = list(state_dicts[0].keys())\n",
    "\n",
    "    # (Optional but recommended) ‚Äî check all clients have the same architecture\n",
    "    for i, sd in enumerate(state_dicts[1:], start=1):\n",
    "        if sd.keys() != state_dicts[0].keys():\n",
    "            raise ValueError(f\"Client {i} has mismatched model parameters ‚Äî all models must share the same keys.\")\n",
    "\n",
    "    # Prepare weights\n",
    "    # If no weights provided ‚Üí use uniform averaging\n",
    "    # Otherwise ‚Üí normalize weights so they sum to 1\n",
    "    num_clients = len(models)\n",
    "    if weights is None:\n",
    "        weights = [1.0 / num_clients] * num_clients\n",
    "    else:\n",
    "        total = float(sum(weights))\n",
    "        weights = [w / total for w in weights]\n",
    "\n",
    "    # Decide device for accumulation (default = CPU)\n",
    "    if device is None:\n",
    "        device = torch.device(\"cpu\")\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # üß† Your turn:\n",
    "    # Implement the weighted averaging logic here.\n",
    "    #\n",
    "    # For each parameter key:\n",
    "    #   ‚Ä¢ Initialize an empty accumulator tensor (zeros_like the first client‚Äôs tensor)\n",
    "    #   ‚Ä¢ For each client:\n",
    "    #       - Move that tensor to the chosen device\n",
    "    #       - Multiply by the client‚Äôs weight\n",
    "    #       - Add to the accumulator\n",
    "    #   ‚Ä¢ Store the accumulated (averaged) tensor in `avg_state_dict`\n",
    "    #\n",
    "    # Hint: Use `torch.zeros_like(...)` for initialization and `.to(device)` \n",
    "    #       to handle device placement.\n",
    "    #\n",
    "    # Return: avg_state_dict\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    ###### ‚úèÔ∏è Write your code below ######\n",
    "    \n",
    "    # avg_state_dict = {}\n",
    "    # for key in keys:\n",
    "    #     ...\n",
    "    # return avg_state_dict\n",
    "\n",
    "    ###### End of student implementation ######\n",
    "\n",
    "    return avg_state  # keep this line for consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Main Federated Training Loop (Server Orchestration)\n",
    "# ------------------------------------------------------------\n",
    "# Each round represents one complete cycle of:\n",
    "#   1. Server sending the global model to all clients\n",
    "#   2. Each client training locally on its own data\n",
    "#   3. Clients returning their updated local models\n",
    "#   4. Server aggregating the local updates (FedAvg)\n",
    "# ============================================================\n",
    "\n",
    "num_rounds = 15    # total federated communication rounds\n",
    "best = 0           # track the best global AUROC during training\n",
    "\n",
    "for round_num in range(num_rounds):\n",
    "    print(f\"\\n================ Round {round_num + 1}/{num_rounds} ================\\n\")\n",
    "\n",
    "    client_samples = []   # store each client‚Äôs locally trained model\n",
    "    LOSS = []             # store local training losses for logging\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Client-side training (simulated serially here for simplicity)\n",
    "    # In real FL, this happens in parallel on different devices\n",
    "    # ------------------------------------------------------------\n",
    "    for client_id in range(nodes):\n",
    "        print(f\"--> Training on Client {client_id + 1}/{nodes}\")\n",
    "\n",
    "        # Initialize a fresh local model and load current global weights\n",
    "        local_model = CNNClassifier(n_features, num_filters, device).to(device)\n",
    "        local_model.load_state_dict(global_model.state_dict())  # sync with server\n",
    "\n",
    "        # Define optimizer and perform local training on this client‚Äôs data\n",
    "        optimizer = torch.optim.Adam(local_model.parameters(), lr=0.001)\n",
    "        local_model, loss = train_model(\n",
    "            local_model,\n",
    "            Loaders[client_id][0],   # train loader for this client\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            device=device,\n",
    "            num_epochs=1 ## number of local epochs\n",
    "        )  ## this function is from Clients.py\n",
    "        LOSS.append(loss)\n",
    "        client_samples.append(local_model)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Server-side aggregation (FedAvg)\n",
    "    # ------------------------------------------------------------\n",
    "    # aggregated_params = federated_averaging(client_samples, weights) ### if you dont pass weights, each client will be given equal importance in the aggregation\n",
    "\n",
    "    # # Update the global model‚Äôs state dictionary with averaged parameters\n",
    "    # new_state_dict = {k: aggregated_params[i] for i, k in enumerate(global_model.state_dict().keys())}\n",
    "    # global_model.load_state_dict(new_state_dict)\n",
    "    # aggregated_state is a dict (state_dict-like) returned by federated_averaging\n",
    "    aggregated_state = federated_averaging(client_samples, weights=weights, device=device)\n",
    "    global_model.load_state_dict(aggregated_state)\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Validate local models (post-training performance on validation) and store best performing one at each client\n",
    "    # ------------------------------------------------------------\n",
    "    for k in range(nodes):\n",
    "        local_model = client_samples[k]\n",
    "        DF[k], Val_AUC[k], cur_auc, cur_apr = evaluate_models(\n",
    "            k, Loaders, local_model, criterion, device,\n",
    "            DF[k], Val_AUC, LOSS[k], 'FedAvg'\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Client {k + 1:02d} | \"\n",
    "            f\"Train Loss: {LOSS[k]:.3f} | \"\n",
    "            f\"Best Val AUC: {Val_AUC[k]:.3f} | \"\n",
    "            f\"Current AUC: {cur_auc:.3f} | \"\n",
    "            f\"Current APR: {cur_apr:.3f}\"\n",
    "        )\n",
    "\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Validate the updated global model on each client‚Äôs test set\n",
    "    # ------------------------------------------------------------\n",
    "    total_auc = 0\n",
    "    for k in range(nodes):\n",
    "        _, cur_auc, cur_apr = evaluate_models_test(k, Loaders, global_model, criterion, device)\n",
    "        total_auc += cur_auc\n",
    "\n",
    "    global_auc = total_auc / nodes\n",
    "    print(f\"\\n>>> Global Model Average AUC (Round {round_num + 1}): {global_auc:.4f}\")\n",
    "\n",
    "    # Save the global model if it achieves a new best AUC\n",
    "    if global_auc > best:\n",
    "        best = global_auc\n",
    "        torch.save(global_model.state_dict(), './trained_models/FedAvg/global_model.pt')\n",
    "    print(\"============================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üîç Evaluation at Each Client\n",
    "# ------------------------------------------------------------\n",
    "# Using the best-performing local models stored during\n",
    "# federated training (FedAvg).\n",
    "# ------------------------------------------------------------\n",
    "# For each client:\n",
    "#   1. Load the locally saved model\n",
    "#   2. Evaluate on that client's test set\n",
    "#   3. Record metrics (AUC, APR)\n",
    "#   4. Save results to CSV\n",
    "# ------------------------------------------------------------\n",
    "# Finally, compute and display average metrics across clients.\n",
    "# ============================================================\n",
    "\n",
    "from utils import *\n",
    "\n",
    "total_auc = 0.0\n",
    "total_apr = 0.0\n",
    "\n",
    "print(\"\\n========== Evaluating Best Local Models ==========\\n\")\n",
    "\n",
    "for client_id in range(nodes):\n",
    "    model_path = f'./trained_models/FedAvg/node{client_id}'\n",
    "    result_path = f'./Results/FedAvg/node{client_id}.csv'\n",
    "\n",
    "    # Load best local model for this client\n",
    "    local_model = torch.load(model_path, map_location=device)\n",
    "    local_model.to(device)\n",
    "    local_model.eval()\n",
    "\n",
    "    # Evaluate on client's test data (index 2 = test loader)\n",
    "    test_loss, test_auc, test_apr = prediction_binary(local_model, Loaders[client_id][2], criterion, device)\n",
    "\n",
    "    total_auc += test_auc\n",
    "    total_apr += test_apr\n",
    "\n",
    "    print(f\"Client {client_id + 1:02d} | \"\n",
    "          f\"Test AUC: {test_auc:.4f} | \"\n",
    "          f\"Test APR: {test_apr:.4f}\")\n",
    "\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Compute and display overall performance\n",
    "# --------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üåç Evaluate the Final Global Model (FedAvg)\n",
    "# ------------------------------------------------------------\n",
    "# The saved global model is evaluated on each client‚Äôs test set\n",
    "# to assess generalization across distributed data.\n",
    "# ------------------------------------------------------------\n",
    "# For each client:\n",
    "#   1. Load the final global model\n",
    "#   2. Evaluate on the client's local test data\n",
    "#   3. Collect AUC and APR metrics\n",
    "# ------------------------------------------------------------\n",
    "# Finally, compute the mean AUC and APR across all clients.\n",
    "# ============================================================\n",
    "\n",
    "sum_auc = 0.0\n",
    "sum_apr = 0.0\n",
    "\n",
    "# Load the best global model saved during training\n",
    "global_model = CNNClassifier(n_features, num_filters, device)   # or the correct class\n",
    "state = torch.load('./trained_models/FedAvg/global_model_state.pt', map_location=device)\n",
    "global_model.load_state_dict(state)\n",
    "global_model.to(device)\n",
    "global_model.eval()\n",
    "\n",
    "print(\"\\n========== Evaluating Final Global Model on All Clients ==========\\n\")\n",
    "\n",
    "for client_id in range(nodes):\n",
    "    # Evaluate global model on each client's test set\n",
    "    test_loss, test_auc, test_apr = prediction_binary(global_model, Loaders[client_id][2], criterion, device)\n",
    "\n",
    "    sum_auc += test_auc\n",
    "    sum_apr += test_apr\n",
    "\n",
    "    print(f\"Client {client_id + 1:02d} | \"\n",
    "          f\"Test AUC: {test_auc:.4f} | \"\n",
    "          f\"Test APR: {test_apr:.4f}\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Compute and print global performance across all clients\n",
    "# ------------------------------------------------------------\n",
    "avg_auc = sum_auc / nodes\n",
    "avg_apr = sum_apr / nodes\n",
    "\n",
    "print(\"\\n================ Global Model Summary ==================\")\n",
    "print(f\"Average Test AUC across {nodes} clients: {avg_auc:.4f}\")\n",
    "print(f\"Average Test APR across {nodes} clients: {avg_apr:.4f}\")\n",
    "print(\"========================================================\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
