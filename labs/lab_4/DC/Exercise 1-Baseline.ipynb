{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß© Baseline Training ‚Äî Full Dataset Model Performance\n",
    "\n",
    "Before we study **dataset condensation**, it‚Äôs important to first train a model on the **full real dataset** and record its performance.  \n",
    "This gives us a *baseline* ‚Äî a reference point against which we can later compare condensed (synthetic) datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "Train a standard deep learning model (e.g., LSTM or RNN) on the complete time-series dataset and evaluate its predictive performance.  \n",
    "The resulting accuracy and calibration metrics serve as a **ground truth** for how well a model can perform when it has access to all real samples.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è What this notebook does\n",
    "\n",
    "1. **Load and normalize** the real dataset  \n",
    "   - Uses the data loader to create training, validation, and test splits.\n",
    "   - Applies standardization or min‚Äìmax normalization.\n",
    "\n",
    "2. **Define and train** a simple baseline model  \n",
    "   - We‚Äôll use an LSTM (or RNN) suited for sequential / temporal data.\n",
    "   - Trained using standard supervised learning on the full training data.\n",
    "\n",
    "3. **Evaluate**  \n",
    "   - Compute **AUC**, **APR**, and **loss** on validation and test sets.  \n",
    "   - Store the best model (based on validation AUC) for later comparison.\n",
    "\n",
    "4. **Visualize**  \n",
    "   - Plot loss curves and validation metrics across epochs to understand convergence and overfitting behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Why a baseline matters\n",
    "\n",
    "When we later perform **dataset condensation**, we‚Äôll train new models on **synthetic data** generated through methods like *logit distribution matching*.  \n",
    "By comparing the condensed model‚Äôs performance to this baseline, we can quantify:\n",
    "\n",
    "- How much accuracy or AUC is lost when training on synthetic instead of real data.\n",
    "- Whether condensation effectively captures the important information in the real dataset.\n",
    "\n",
    "---\n",
    "\n",
    "üëâ **In short:**  \n",
    "This notebook establishes a **performance benchmark** using real data.  \n",
    "All future condensation experiments will be evaluated relative to this baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import modules\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from loaders import get_loaders_time_series   # replace with actual module name if different\n",
    "from utils_1 import get_device, prediction_binary\n",
    "from models import LSTMClassifier   # replace if your LSTM class is in a different module\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)  # Python random module\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)  # Numpy random module\n",
    "    torch.manual_seed(seed)  # PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)  # PyTorch GPU (single GPU)\n",
    "    torch.cuda.manual_seed_all(seed)  # PyTorch GPU (all GPUs)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "rand_seed = 42\n",
    "set_seed(rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Load data (adjust `path` to where your pickles are)\n",
    "# -------------------------\n",
    "data_path = \"../DATA/\"   # <-- change to your data folder\n",
    "train_loader, val_loader, test_loader = get_loaders_time_series(\n",
    "    path=data_path,\n",
    "    train_batch=128,\n",
    "    val_batch=256,\n",
    "    test_batch=256,\n",
    "    sampler=True,\n",
    "    pre_process=\"std\",\n",
    "    ds_half=0,\n",
    ")\n",
    "\n",
    "# Inspect one batch shape to infer model input dims\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(\"One batch x shape:\", batch_x.shape, \"y shape:\", batch_y.shape)\n",
    "# typical shape: (batch, seq_len, n_features)\n",
    "\n",
    "seq_len = batch_x.shape[1]\n",
    "n_features = batch_x.shape[2]\n",
    "print(f\"seq_len={seq_len}, n_features={n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Create a simple LSTM model\n",
    "# -------------------------\n",
    "# LSTMClassifier should accept (input_dim, hidden_dim,device, output dim).\n",
    "hidden_dim = 32\n",
    "model = LSTMClassifier(input_dim=n_features, hidden_dim=hidden_dim, device=device, output_dim=1)\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()   # use logits from model, no sigmmoid activation in model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Training settings (simple)\n",
    "# -------------------------\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_aucs = []\n",
    "val_aprs = []\n",
    "\n",
    "# Helper: evaluate average training loss (quick)\n",
    "def evaluate_train_loss(loader):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(torch.float32).to(device)\n",
    "            y = y.to(torch.float32).to(device)\n",
    "            out = model(x)[:, 0]\n",
    "            loss = loss_fn(out, y)\n",
    "            total += loss.item()\n",
    "            count += 1\n",
    "    return total / max(1, count)\n",
    "\n",
    "# Initial evaluation (before training)\n",
    "train_losses.append(evaluate_train_loss(train_loader))\n",
    "val_loss, val_auc, val_apr = prediction_binary(model, val_loader, loss_fn, device)\n",
    "val_losses.append(val_loss); val_aucs.append(val_auc); val_aprs.append(val_apr)\n",
    "print(f\"Init ‚Äî Train loss: {train_losses[-1]:.4f}, Val AUC: {val_auc:.4f}, Val APR: {val_apr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Choose which metric to use for saving best model:\n",
    "# Options: \"val_auc\" (higher is better) or \"train_loss\" (lower is better)\n",
    "# -------------------------\n",
    "monitor_metric = \"val_auc\"   # set to \"train_loss\" if you prefer\n",
    "best_metric = -float(\"inf\") if monitor_metric == \"val_auc\" else float(\"inf\")\n",
    "best_epoch = -1\n",
    "best_path = \"./best_model_state.pt\"   # saved as state_dict (recommended)\n",
    "\n",
    "# -------------------------\n",
    "# Training loop with checkpointing\n",
    "# -------------------------\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(torch.float32).to(device)\n",
    "        y = y.to(torch.float32).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x)[:, 0]         # assume model returns shape (B, 1) or (B,)\n",
    "        loss = loss_fn(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # ---------- validation ----------\n",
    "    val_loss, val_auc, val_apr = prediction_binary(model, val_loader, loss_fn, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_aucs.append(val_auc)\n",
    "    val_aprs.append(val_apr)\n",
    "\n",
    "    # ---------- decide whether to save best model ----------\n",
    "    save_now = False\n",
    "    if monitor_metric == \"val_auc\":\n",
    "        if val_auc > best_metric:\n",
    "            best_metric = val_auc\n",
    "            save_now = True\n",
    "    elif monitor_metric == \"train_loss\":\n",
    "        if avg_train_loss < best_metric:\n",
    "            best_metric = avg_train_loss\n",
    "            save_now = True\n",
    "    else:\n",
    "        raise ValueError(\"monitor_metric must be 'val_auc' or 'train_loss'\")\n",
    "\n",
    "    if save_now:\n",
    "        best_epoch = epoch\n",
    "        # save state_dict (more portable than saving the full model)\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        print(f\"--> Saved new best model (epoch={epoch}) | {monitor_metric} = {best_metric:.4f}\")\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{num_epochs} ‚Äî Train loss: {avg_train_loss:.4f} | \"\n",
    "        f\"Val loss: {val_loss:.4f} | Val AUC: {val_auc:.4f} | Val AUPRC: {val_apr:.4f}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# After training: load best model and evaluate on test set\n",
    "# -------------------------\n",
    "if best_epoch == -1:\n",
    "    print(\"No checkpoint was saved during training. Evaluating final model.\")\n",
    "    best_model_state = None\n",
    "else:\n",
    "    print(f\"\\nLoading best model from epoch {best_epoch} (saved at '{best_path}').\")\n",
    "    best_model_state = torch.load(best_path, map_location=device)\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "# Ensure model is in eval mode for testing\n",
    "model.eval()\n",
    "test_loss, test_auc, test_apr = prediction_binary(model, test_loader, loss_fn, device)\n",
    "\n",
    "print(\"\\n====== Best model test evaluation ======\")\n",
    "if monitor_metric == \"val_auc\":\n",
    "    print(f\"Best Val AUC (used for checkpoint) = {best_metric:.4f} (epoch {best_epoch})\")\n",
    "else:\n",
    "    print(f\"Best Train Loss (used for checkpoint) = {best_metric:.4f} (epoch {best_epoch})\")\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test AUC: {test_auc:.4f} | Test APR: {test_apr:.4f}\")\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Simple plots\n",
    "# -------------------------\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch (including init)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss curve\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(val_aucs, label=\"Val AUC\", marker='x')\n",
    "plt.plot(val_aprs, label=\"Val APR\", marker='o')\n",
    "plt.xlabel(\"Epoch (including init)\")\n",
    "plt.legend()\n",
    "plt.title(\"Validation metrics\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
