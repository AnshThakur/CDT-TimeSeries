{"cells":[{"cell_type":"markdown","metadata":{"id":"zfqERVVKxIU0"},"source":["# Implement RNN unfolding using RNNCell and LSTMCell\n","\n","<br>\n","<font color='#2c3e50'>\n","    \n","- RNN, LSTM and GRU cells can be seen as neural networks that process each step of the input time-series (in somewhat recursive manner). This is often referred to as `unrolling or unfolding of RNN`.\n","- LSTM/RNN/GRU layers provide an abstraction over this operation.\n","</font>\n","\n","## 1. Task:\n","\n","<font color='red'>\n","\n","<b>In this lab exercise, you have to implement `unfolding or unrolling` using `RNNCell` and `LSTMCell`. </b> \n","</font>\n","\n","\n","<font color='#2c3e50'>  \n","<ul>    \n","<li>\n","\n","RNNCell (or LSTMCell) takes a feature vector and hidden state (or hidden and cell state) at a time-step and outputs the updated hidden state (or hidden and cell states). The hidden state obtained after the last time-step is considered as a latent representation of the time-series and is given as input to the dense or classification layer(s). </li>   \n","    \n","<li>\n","    \n","This notebook contains two PyTorch model definitions (i.e. `LSTMClassifier` and `RNNClassifier`) and `trainer()` function to process the training data and train models. The entire code is in `almost` working condition. You need to complete the `forward()` function in both `LSTMClassifier` and `RNNClassifier` classes. In essence, you need to implement RNN unrolling in `forward()`. </li>   \n","\n","    \n","<li>\n","    \n","If you are not comfortable with PyTorch, please see the Reference notebook. It summarises how to define and train a model in modular manner. </li>\n","    \n","\n","<li> \n","    \n","Since RNN cells are essentially composed of standard dense or linear layers, you are welcome to implement your own RNNCell instead of using the implemented one.  \n","    \n","    \n","</ul>\n","</font>\n","    \n"," \n","### 1.1 HINTS:\n","<br>\n","<font color='#2c3e50'>  \n","<ul>    \n","<li>\n","    \n","Run a loop over time-steps </li>\n","\n","<li>\n","    \n","Neural network process one batch at a time. So, input to `RNNCell` at each time-step is `(batch_size,n_features)`</li>    \n","</ul>\n","</font>\n","\n","### 1.2 Instructions:\n","\n","- You have to implement `forward functions` in both `LSTMClassifier` and `RNNClassifier` classes. \n","- These functions take a batch of time-series of size `(batch_size,n_steps,n_features)` and output mortality predictions (1/0 binary predictions) of shape `(batch_size,1)`.\n","\n","\n","### 1.3 Data:\n","\n","- Physionet 2012: Mortality prediction (discussed yesterday)\n","- Each time-series consists of K-dimenional vector (`n_features=K`) at 48 time-steps (`n_steps=48`). `Size of each time-series : (48,K)`\n"]},{"cell_type":"markdown","metadata":{"id":"hdc1v7fvxIU5"},"source":["## 2. Synatx of RNNCell and LSTMCell\n","\n","\n","### 2.1 RNNCell:\n","\n","\n","#### PyTorch syntax:\n","<img src=\"./img/rnn_cell_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n","<img src=\"./img/rnn_cell_2.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n","<img src=\"./img/rnn_cell_3.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n","<img src=\"./img/rnn_cell_4.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n","<img src=\"./img/rnn_cell_5.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n","\n","### 2.2 LSTMCell\n","\n","#### PyTorch syntax:\n","<img src=\"./img/lstm_cell_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n","<img src=\"./img/lstm_cell_2.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n","<img src=\"./img/lstm_cell_3.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n","<img src=\"./img/lstm_cell_4.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n","\n","\n","\n","### 2.3 More information at:\n","\n","- http://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html\n","- http://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html"]},{"cell_type":"markdown","metadata":{"id":"sw9jhQgQxIU6"},"source":["## Implementation Begins:"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1188,"status":"ok","timestamp":1636212379714,"user":{"displayName":"Anshul Thakur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11719449395424920327"},"user_tz":0},"id":"8Ust8VKMxIU7"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from tqdm import tqdm\n","import numpy as np\n","from sklearn.metrics import roc_auc_score\n","from utils import get_validation_score\n","import warnings\n","warnings.filterwarnings('ignore')\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1636212379715,"user":{"displayName":"Anshul Thakur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11719449395424920327"},"user_tz":0},"id":"f39vXbYTxIU9"},"outputs":[],"source":["class RNNClassifier(nn.Module):\n","    \n","    def __init__(self, input_dim, hidden_dim,device):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","        self.rnn = nn.RNNCell(input_dim, hidden_dim)  # RNN Cell\n","        self.fc = nn.Linear(hidden_dim, 1)            # fully connected layer: maps last hidden vector to model prediction\n","        self.activation = nn.Sigmoid()                # coz binary classification\n","        self.device=device\n","    \n","    def forward(self, x):\n","\n","        hidden = self.init_hidden(x)\n","        \n","        ############################# \n","        \n","        # Write you code here.\n","        # Return expects variable out. Its the hidden vector obtained after last time-step.\n","        \n","        time_steps=x.shape[1]                 # shape of x is (batches,time_Steps,features)\n","        \n","        for i in range(0,time_steps):\n","            inputs=x[:,i]                     # (batch,features) shape\n","            hidden = self.rnn(inputs,hidden)\n","            \n","        out = self.fc(hidden)                 # take the hidden vector corresponding to last time step\n","        ###########################\n","        \n","        return self.activation(out)\n","    \n","    def init_hidden(self, x):\n","        h0 = torch.zeros(x.size(0), self.hidden_dim)\n","        return h0.to(self.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1636212379716,"user":{"displayName":"Anshul Thakur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11719449395424920327"},"user_tz":0},"id":"ikvKS54axIU-"},"outputs":[],"source":["class LSTMClassifier(nn.Module):\n","    \n","    def __init__(self, input_dim, hidden_dim,device):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","        self.rnn = nn.LSTMCell(input_dim, hidden_dim)\n","        self.fc = nn.Linear(hidden_dim, 1)\n","        self.activation = nn.Sigmoid()\n","        self.device=device\n","    \n","    def forward(self, x):\n","        hidden,cell = self.init_hidden(x)\n","        \n","        ############################\n","        \n","        # Write you code here.\n","        # Resturn expects variable out. Its the hidden vector obtained after last time-step.\n","        \n","        \n","        time_steps=x.shape[1]              #shape of x is (batches,time_Steps,features)\n","        \n","        for i in range(0,time_steps):\n","            inputs=x[:,i]                  # (batch,features) shape\n","            hidden,cell = self.rnn(inputs,(hidden,cell))\n","        \n","        out = self.fc(hidden)              # take the hidden vector corresponding to last time step\n","        #############################\n","        \n","        return self.activation(out)\n","    \n","    def init_hidden(self, x):\n","        h0 = torch.zeros(x.size(0), self.hidden_dim)\n","        c0 = torch.zeros(x.size(0), self.hidden_dim)\n","        return h0.to(self.device),c0.to(self.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1636212379717,"user":{"displayName":"Anshul Thakur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11719449395424920327"},"user_tz":0},"id":"Pdk6nqo8xIU-"},"outputs":[],"source":["# Function to compute validation score: Used in trainer()\n","\n","def get_validation_score(model,Val_T,Val_L):\n","    model.eval()\n","    tensor_x = torch.Tensor(Val_T).to(model.device)\n","    preds=model(tensor_x)[:,0]\n","    LOSS=nn.BCELoss().to(device)\n","    val_loss=LOSS(preds,torch.Tensor(Val_L).type(torch.FloatTensor).to(model.device))\n","    return roc_auc_score(Val_L, preds.cpu().detach().numpy()), val_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1636212379718,"user":{"displayName":"Anshul Thakur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11719449395424920327"},"user_tz":0},"id":"qjKFBLTyxIU_"},"outputs":[],"source":["def trainer(model,training_set,validation_set,device,lr,stored_name,epochs=10):\n","    \n","    # Recieves data and labels \n","    T,L=training_set \n","    Val_T,Val_L=validation_set\n","    \n","    # intialise optimiser and criterion\n","    \n","    optimizer_model = torch.optim.SGD(model.parameters(),lr,momentum=0.9, nesterov=True)\n","    criterion = nn.BCELoss().to(device)\n","\n","    # \n","    best=0\n","    LOSS=[]\n","    VAL_LOSS=[]\n","    \n","    # training begins\n","    \n","    for epoch in range(0,epochs):\n","        Loss=0\n","        model.train()\n","        for k in range(0,len(T)):\n","            \n","            inputs=T[k]\n","            labels=L[k]\n","            \n","            inputs=torch.Tensor(inputs).to(device)\n","            labels=torch.Tensor(labels).type(torch.FloatTensor).to(device)\n","            \n","            pred=model(inputs)\n","            \n","            loss=criterion(pred[:,0],labels)\n","            optimizer_model.zero_grad()\n","            loss.backward()\n","            optimizer_model.step()\n","            Loss=Loss+loss\n","           \n","        Val_ROC,val_loss=get_validation_score(model,Val_T,Val_L)\n","        VAL_LOSS.append(val_loss.detach().cpu().numpy())\n","        LOSS.append((Loss/len(T)).detach().cpu().numpy())\n","        \n","        print(' Epoch: {:.1f} Training Loss {:5f} Validation Loss {:.4f} Validation AUC {:.5f}'.format(epoch,LOSS[-1],VAL_LOSS[-1],Val_ROC))\n","        \n","        # If current validation score is greater than best, store the model\n","        \n","        if best<Val_ROC:\n","           torch.save(model, './'+stored_name) \n","\n","    return torch.load('./'+stored_name).to(device),LOSS,VAL_LOSS     "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":845,"status":"ok","timestamp":1636212380550,"user":{"displayName":"Anshul Thakur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11719449395424920327"},"user_tz":0},"id":"8itcyfT3xIVA","outputId":"a8415f54-8b80-4f65-c715-a0d00320c399"},"outputs":[],"source":["# Get training and validation data\n","\n","from get_data import get_training_data,get_validation_data\n","\n","T,L=get_training_data(batch_size=32)  # returns lists of training data and label batches\n","Val_T,Val_L=get_validation_data()     # numpy arrays of validation data and labels\n","\n","print(T[0].shape)                     # (batch_size,time_steps,n_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1636212380551,"user":{"displayName":"Anshul Thakur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11719449395424920327"},"user_tz":0},"id":"y43nY3esxIVC"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1636212380552,"user":{"displayName":"Anshul Thakur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11719449395424920327"},"user_tz":0},"id":"wrOne0ySxIVC"},"outputs":[],"source":["n_features=T[0].shape[2]     # 76 dimensional vector at each time step\n","recurrent_units=128          # number of hidden units in  a RNN/LSTM\n","lr=0.001                     # learning rate "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":13,"status":"ok","timestamp":1636212380553,"user":{"displayName":"Anshul Thakur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11719449395424920327"},"user_tz":0},"id":"NnxeESThxIVD","outputId":"69d9927a-96aa-49ef-e904-e980717c0396"},"outputs":[],"source":["# Create LSTMClassifier model object\n","\n","model=LSTMClassifier(n_features,recurrent_units,device)\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":82127,"status":"ok","timestamp":1636212462671,"user":{"displayName":"Anshul Thakur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11719449395424920327"},"user_tz":0},"id":"dGsbdDO9xIVD","outputId":"9910184d-d6d3-4703-bda4-dfc8e44f0eda"},"outputs":[],"source":["# Train LSTMClassifier\n","\n","model=model.to(device)\n","model, training_loss, validation_loss=trainer(model,(T,L),(Val_T,Val_L),device,lr,stored_name='lstm',epochs=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":404,"status":"ok","timestamp":1636212463051,"user":{"displayName":"Anshul Thakur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11719449395424920327"},"user_tz":0},"id":"dNz56XhfxIVE","outputId":"96fea02f-60c8-47db-afe0-0571394183d8"},"outputs":[],"source":["# Plot training and validation loss\n","import matplotlib.pyplot as plt\n","\n","fig, ax = plt.subplots(figsize=(5,5))\n","\n","lw = 2\n","\n","plt.tight_layout()\n","ax.plot(np.linspace(1, len(training_loss), num=len(training_loss)),training_loss, color='rebeccapurple',\n","         lw=2, linestyle='-', label='Training Loss')\n","\n","ax.plot(np.linspace(1, len(training_loss), num=len(training_loss)),validation_loss, color='r',\n","         lw=2, linestyle='-', label='Validation_loss')\n","\n","ax.set_xlabel('# Epochs',fontsize=14)\n","ax.set_ylabel('# Loss',fontsize=14)\n","ax.legend(loc=\"best\",fontsize=12)\n","\n","ax.tick_params(axis='x', labelsize=13)\n","ax.tick_params(axis='y', labelsize=13)\n","\n","\n","plt.grid(color='gray', linestyle='-', linewidth=0.1)\n","\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":24,"status":"ok","timestamp":1636212463052,"user":{"displayName":"Anshul Thakur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11719449395424920327"},"user_tz":0},"id":"hDbJz7X5xIVE","outputId":"f3f93b10-2890-4cb6-87f6-6444f86b35b4"},"outputs":[],"source":["# Create RNNClassifier model object\n","\n","model=RNNClassifier(n_features,recurrent_units,device)\n","print(model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25245,"status":"ok","timestamp":1636212488277,"user":{"displayName":"Anshul Thakur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11719449395424920327"},"user_tz":0},"id":"rfH4BX_TxIVE","outputId":"61f99dbe-2714-4dee-f0fa-f13185452959"},"outputs":[],"source":["# Train RNNClassifier\n","\n","model=model.to(device)\n","model, training_loss, validation_loss=trainer(model,(T,L),(Val_T,Val_L),device,lr,stored_name='rnn',epochs=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":386},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1636212488279,"user":{"displayName":"Anshul Thakur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11719449395424920327"},"user_tz":0},"id":"Mu9R8dUAxIVF","outputId":"88a70b17-dff8-4eed-abd5-eaf2b9503f95"},"outputs":[],"source":["# Plot training and validation loss\n","\n","fig, ax = plt.subplots(figsize=(5,5))\n","\n","lw = 2\n","\n","plt.tight_layout()\n","ax.plot(np.linspace(1, len(training_loss), num=len(training_loss)),training_loss, color='rebeccapurple',\n","         lw=2, linestyle='-', label='Training Loss')\n","\n","ax.plot(np.linspace(1, len(training_loss), num=len(training_loss)),validation_loss, color='r',\n","         lw=2, linestyle='-', label='Validation_loss')\n","\n","ax.set_xlabel('# Epochs',fontsize=14)\n","ax.set_ylabel('# Loss',fontsize=14)\n","ax.legend(loc=\"best\",fontsize=12)\n","\n","ax.tick_params(axis='x', labelsize=13)\n","ax.tick_params(axis='y', labelsize=13)\n","\n","\n","plt.grid(color='gray', linestyle='-', linewidth=0.1)\n","\n","\n","#plt.savefig('./loss_curves.pdf',dpi=100,bbox_inches='tight')\n","plt.show()"]}],"metadata":{"colab":{"name":"Problem 1  with solution.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.9.7 ('ml4timeseries')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"c93591c313a95c8a89c667763bfd86bbb342a6e2dd3e91f10558630342cde4f4"}}},"nbformat":4,"nbformat_minor":0}
